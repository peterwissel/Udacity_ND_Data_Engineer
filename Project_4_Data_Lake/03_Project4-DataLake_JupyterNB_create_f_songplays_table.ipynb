{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# creating table f_songplays\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "Create table f_songplays:\n",
    "\n",
    "- Join the source data frames \"df_log_table\" and \"df_songs_table\" from file \"Project4-DataLake_JupyterNB_process_song_data.ipynb\")\n",
    "  together by the following columns:\n",
    "    - df_log_table.artist = df_songs_table.artist_name\n",
    "    - df_log_table.song = df_songs_table.title\n",
    "- Filter dataset by \"df_log_table.page = 'NextSong'\" to reduce loaded data to the important entries for analysis\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "************************************************************************************************************************\n",
    "Import needed data from data_frame df_songs_table\n",
    "************************************************************************************************************************\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "This file in only for developing purposes, because it's not needed to create a Spark Session each time\n",
    "\"\"\"\n",
    "import configparser\n",
    "import os\n",
    "from pyspark.sql import SparkSession, Window"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set global variables to access S3 via OS environment variables\n",
    "\"\"\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\"\"\"\n",
    "Create Spark Session and setting LOG-Level\n",
    "Attributes:\n",
    "    ERROR   - less details - only when something went wrong\n",
    "    WARN    - more details\n",
    "    INFO    - more details than Warn level\n",
    "    DEBUG   - very detailed information\n",
    "\"\"\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"etl pipeline for project 4 - Data Lake\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# setting the current LOG-Level\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load data frame from former written PARQUET file\n",
    "Data to write: d_songs\n",
    "\"\"\"\n",
    "df_songs_table = spark.read.parquet('s3a://project-4-data-lake/analytics/staging/df_songs_table.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: integer (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_songs_table.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load data frame from former written PARQUET file\n",
    "Data to write: d_songs\n",
    "\"\"\"\n",
    "\n",
    "df_logs_table = spark.read.parquet('s3a://project-4-data-lake/analytics/staging/df_logs_table.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_logs_table.printSchema()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All entries: 15\n",
      "Filter by 'NextSong (only count)'11\n",
      "dropDuplicates: 15\n",
      "dropna: 11\n",
      "All entries: 15\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prepare f_songplays Fact table\n",
    "\n",
    "# Tutorial for a JOIN with Data Frames: http://www.learnbymarketing.com/1100/pyspark-joins-by-example/\n",
    "LEFT ANTI JOIN\n",
    "\"\"\"\n",
    "\n",
    "# check loaded data\n",
    "print(\"All entries: \" + str(df_logs_table.count()))\n",
    "print(\"Filter by 'NextSong (only count)'\" + str(df_logs_table.filter(df_logs_table.page == \"NextSong\").count()))\n",
    "print(\"dropDuplicates: \" + str(df_logs_table.dropDuplicates().count()))\n",
    "print(\"dropna: \" + str(df_logs_table.dropna().count()))\n",
    "print(\"All entries: \" + str(df_logs_table.count()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# create an alias, because this is more handy for the later use\n",
    "df_s = df_songs_table.alias('df_s')\n",
    "df_l = df_logs_table.alias('df_l')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# filter data set to pass only \"df_l.page == \"NextSong\" and then join both data sets together by \"left join\"\n",
    "df_f_songplays_joined = df_l \\\n",
    "    .filter(df_l.page == \"NextSong\") \\\n",
    "    .join(df_s, [(df_l.artist == df_s.artist_name) & (df_l.song == df_s.title)], how='left') \\\n",
    "    .withColumnRenamed(\"ts_timestamp\", \"start_time\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, hour, weekofyear, dayofweek, dayofmonth, date_format, to_timestamp, \\\n",
    "    row_number, monotonically_increasing_id\n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table\n",
    "# songplay_id consists of a window function which is sorted by monotonically_increasing_id()\n",
    "df_f_songplays = df_f_songplays_joined \\\n",
    "    .withColumnRenamed(\"userid\", \"user_id\") \\\n",
    "    .withColumnRenamed(\"sessionid\", \"session_id\") \\\n",
    "    .withColumnRenamed(\"useragent\", \"user_agent\") \\\n",
    "    .withColumn(\"year\", year(df_f_songplays_joined.start_time)) \\\n",
    "    .withColumn(\"month\", month(df_f_songplays_joined.start_time)) \\\n",
    "    .withColumn(\"songplay_id\", row_number().over(Window.orderBy(monotonically_increasing_id()))) \\\n",
    "    .select(\"songplay_id\", \"user_id\", \"song_id\", \"artist_id\", \"start_time\", \"session_id\", \"level\", \"location\",\n",
    "            \"user_agent\", \"year\", \"month\") \\\n",
    "    .distinct()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- songplay_id: integer (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- session_id: long (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "+-----------+-------+-------+---------+-------------------+----------+-----+--------------------+--------------------+----+-----+\n",
      "|songplay_id|user_id|song_id|artist_id|         start_time|session_id|level|            location|          user_agent|year|month|\n",
      "+-----------+-------+-------+---------+-------------------+----------+-----+--------------------+--------------------+----+-----+\n",
      "|          1|      8|   null|     null|2018-11-01 22:00:46|       139| free|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "|          2|      8|   null|     null|2018-11-01 22:00:52|       139| free|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "|          3|      8|   null|     null|2018-11-01 22:00:16|       139| free|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "|          4|      8|   null|     null|2018-11-01 22:00:13|       139| free|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "|          5|      8|   null|     null|2018-11-01 22:00:33|       139| free|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "|          6|      8|   null|     null|2018-11-01 22:00:53|       139| free|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "|          7|      8|   null|     null|2018-11-01 22:00:54|       139| free|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "|          8|     10|   null|     null|2018-11-01 22:00:00|         9| free|Washington-Arling...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "|          9|     26|   null|     null|2018-11-01 22:00:05|       169| free|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|2018|   11|\n",
      "|         10|     26|   null|     null|2018-11-01 22:00:25|       169| free|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|2018|   11|\n",
      "+-----------+-------+-------+---------+-------------------+----------+-----+--------------------+--------------------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f_songplays.printSchema()\n",
    "df_f_songplays.show(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# check results (user: 15 = 463; 29 = 346)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f_songplays \\\n",
    "    .filter(df_f_songplays.user_id == 29) \\\n",
    "    .distinct() \\\n",
    "    .groupBy(df_f_songplays.user_id).count() \\\n",
    "    .show()\n",
    "\n",
    "# check results (user: 15 = 429; 29 = 309)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "df_f_songplays \\\n",
    "    .repartition(1) \\\n",
    "    .write \\\n",
    "    .mode(saveMode='Overwrite') \\\n",
    "    .partitionBy(\"year\",\"month\") \\\n",
    "    .parquet('s3a://project-4-data-lake/analytics/f_songplays/f_songplays.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}