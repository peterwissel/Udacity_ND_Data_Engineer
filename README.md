# Udacity Nanodegree Program - Data Engineer

In this course I learned to design data models, built data warehouses and data lakes, automated data pipelines, and 
worked with massive datasets. At the end of the program, I combined my new skills by completing a capstone project.

### Educational Objectives: 
The student (me) learned to
- Create user-friendly relational and NoSQL data models
- Create scalable and efficient data warehouses
- Work efficiently with massive datasets        
- Build and interact with a cloud-based data lake        
- Automate and monitor data pipelines
- Develop proficiency in Spark, Airflow, and AWS tools

For a detailed explanation about this course click 
[here](https://d20vrrgs8k4bvw.cloudfront.net/documents/en-US/Data+Engineering+Nanodegree+Program+Syllabus.pdf).

## The Projects

#### P1 - Data Modeling with Postgres

In this project, I modeled user activity data for a music streaming app called Sparkify. I created a relational 
database and ETL pipeline designed to optimize queries for understanding what songs users are listening to. In 
PostgreSQL I also defined Fact and Dimension tables and inserted data into new tables.

#### P2 - Data Modeling with Apache Cassandra

In this project, I modeled user activity data for a music streaming app called Sparkify. I createed a database and ETL 
pipeline, in both Postgres and Apache Cassandra, designed to optimize queries for understanding what songs users are 
listening to. For PostgreSQL, I also defined Fact and Dimension tables and inserted data into new tables. 
For Apache Cassandra, I modeled data so I ran specific queries provided by the analytics team at Sparkify.

#### P3 - Cloud Data Warehouse with Redshift on AWS

In this project, I was tasked with building an ELT pipeline that extracts data from S3, stages them in Redshift, and 
transforms data into a set of dimensional tables for the analytics team to continue finding insights in what songs the 
users are listening to.

#### P4 - Spark and Data Lakes on AWS

In this project, I built an ETL pipeline for a data lake. The data resides in S3, in a directory of JSON logs on user 
activity on the app, as well as a directory with JSON metadata on the songs in the app. I loaded data from S3, processed 
the data into analytics tables using Spark, and loaded them back into S3. I deployed this Spark process on a cluster 
using AWS.

#### P5 - Automate Data Pipelines with Apache Airflow

In this project, I continued my work on the music streaming company’s data infrastructure by creating and automating a 
set of data pipelines. I configured and scheduled data pipelines with Airflow and monitored and debugged production 
pipelines.

#### P6 - Data Engineering Capstone Project

The purpose of the data engineering capstone project is to give me a chance to combine what I’ve learned throughout the 
program. This project will be an important part of my portfolio that will help me achieve my data engineering-related 
career goals.In this project, **I’ll define** the scope of the project and the data I’ll be working with. We’ll provide 
guidelines, suggestions, tips, and resources to help me to be successful, but my project will be unique to me. 
I’ll gather data from several different data sources ;transform, combine, and summarize it; and create a clean database 
for others to analyze.

