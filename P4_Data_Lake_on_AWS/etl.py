import configparser
import datetime
import os
from typing import Dict

from pyspark.sql import SparkSession, Window
from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str \
    , IntegerType as Int, LongType as Lng, TimestampType

from pyspark.sql.functions import row_number, monotonically_increasing_id
from pyspark.sql.functions import year, month, hour, weekofyear, dayofweek, dayofmonth, date_format, to_timestamp

"""
Set global variables to access S3 via OS environment variables
"""
config = configparser.ConfigParser()
config.read_file(open('dl.cfg'))

os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']
os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']
print("Set AWS_ACCESS_KEY_ID = " + os.getenv('AWS_ACCESS_KEY_ID'))
print("Set AWS_SECRET_ACCESS_KEY = " + os.getenv('AWS_SECRET_ACCESS_KEY'))

"""
**************************************************************
********* useful supporting procedures and functions *********
**************************************************************
"""


def time_tracker(method_name, execution_start, execution_end):
    """
    Procedure to inform the user about the execution time and duration
    :param method_name: STRING - Name of the executed code to distinguish between different executions
    :param execution_start: DATETIME - datetime.datetime.now() --> Return the current local date and time
    :param execution_end: DATETIME - datetime.datetime.now() --> Return the current local date and time
    :return: STRING - Listing of start and end time including duration
    """
    execution_duration = execution_end - execution_start
    print("")
    print("--------------------------------------")
    print("Method: {}".format(method_name))
    print("Total : Start: {}".format(str(execution_start.strftime('%Y-%m-%d %H:%M:%S'))))
    print("Total : End: {}".format(str(execution_end.strftime('%Y-%m-%d %H:%M:%S'))))
    print("Total : Duration: {}".format(str(execution_duration)))
    print("--------------------------------------")


"""
********* main program *********
"""


def create_spark_session():
    """
    Create a Spark session with needed packages. Set log level to ERROR, INFO or DEBUG
    :return: spark session
    """

    # "time_tracker" - timestamp to track running time
    total_execution_start = datetime.datetime.now()
    print("Started create_spark_session(): " + str(total_execution_start))

    # create a new spark session
    spark = SparkSession \
        .builder \
        .appName("etl pipeline for project 4 - Data Lake") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:2.7.0") \
        .getOrCreate()

    # set debug level to ERROR
    spark.sparkContext.setLogLevel('ERROR')

    # "time_tracker"
    total_execution_end = datetime.datetime.now()
    time_tracker("Data_Lake/etl.py/create_spark_session()", total_execution_start, total_execution_end)

    return spark


def set_up_environment_variables(environment_to_use):
    """
    Configure INPUT and OUTPUT path's from "dl.cfg"-file. Distinguish between Cloud and local configuration.
    INPUT_DATA:
        SONG_DATA - Song dataset - Subset of real data from Million Song Dataset.
        LOG_DATA - Log dataset - This dataset consists of log files in JSON format generated by event simulator bases
        on the songs of the Song dataset

    OUTPUT_DATA:
        Table files of the star schema for the Sparkify DB

        d_songs - songs in music database
        d_artists - artists in music database
        d_users - users in the app
        d_time - timestamps of records in songplays broken down into specific units

        f_songplays - records in log data associated with song plays i.e. records with page NextSong

    :param environment_to_use:
    :return: return_environment_variables Dict[str, str]
    """

    """
    How to use python Dictionaries
    https://openbookproject.net/thinkcs/python/english3e/dictionaries.html
    """

    # set variables for current function
    current_environment = environment_to_use
    return_environment_variables: Dict[str, str] = {}

    # INPUT - DATA
    # Build general environment variables
    cloud_prefix = config['GENERAL']['CLOUD_PREFIX']
    cloud_cluster_worker_nodes = config['GENERAL']['CLOUD_CLUSTER_WORKER_NODES']
    local_cluster_worker_nodes = config['GENERAL']['LOCAL_CLUSTER_WORKER_NODES']

    cloud_input_data_bucket_name = config['GENERAL']['CLOUD_INPUT_DATA_BUCKET_NAME']
    local_input_data_path = config['GENERAL']['LOCAL_INPUT_DATA_PATH']

    # Build specific environment variables - SONG_DATA
    cloud_input_song_data_path = config['INPUT_SONG_DATA']['CLOUD_INPUT_SONG_DATA_PATH']
    local_input_song_data_path = config['INPUT_SONG_DATA']['LOCAL_INPUT_SONG_DATA_PATH']
    cloud_input_song_data_path_specific = config['INPUT_SONG_DATA']['CLOUD_INPUT_SONG_DATA_PATH_SPECIFIC']
    local_input_song_data_path_specific = config['INPUT_SONG_DATA']['LOCAL_INPUT_SONG_DATA_PATH_SPECIFIC']
    cloud_input_song_data_file_name = config['INPUT_SONG_DATA']['CLOUD_INPUT_SONG_DATA_FILE_NAME']
    local_input_song_data_file_name = config['INPUT_SONG_DATA']['LOCAL_INPUT_SONG_DATA_FILE_NAME']

    # Build specific environment variables - LOG_DATA
    cloud_input_log_data_path = config['INPUT_LOG_DATA']['CLOUD_INPUT_LOG_DATA_PATH']
    local_input_log_data_path = config['INPUT_LOG_DATA']['LOCAL_INPUT_LOG_DATA_PATH']
    cloud_input_log_data_path_specific = config['INPUT_LOG_DATA']['CLOUD_INPUT_LOG_DATA_PATH_SPECIFIC']
    local_input_log_data_path_specific = config['INPUT_LOG_DATA']['LOCAL_INPUT_LOG_DATA_PATH_SPECIFIC']
    cloud_input_log_data_file_name = config['INPUT_LOG_DATA']['CLOUD_INPUT_LOG_DATA_FILE_NAME']
    local_input_log_data_file_name = config['INPUT_LOG_DATA']['LOCAL_INPUT_LOG_DATA_FILE_NAME']

    # OUTPUT - DATA
    cloud_output_data_bucket_name = config['GENERAL']['CLOUD_OUTPUT_DATA_BUCKET_NAME']
    cloud_output_path = config['GENERAL']['CLOUD_OUTPUT_PATH']
    local_output_data_path = config['GENERAL']['LOCAL_OUTPUT_DATA_PATH']

    # Build output environment variables - DIMENSION SONGS
    cloud_output_dimension_songs_path = config['OUTPUT']['CLOUD_OUTPUT_DIMENSION_SONGS_PATH']
    local_output_dimension_songs_path = config['OUTPUT']['LOCAL_OUTPUT_DIMENSION_SONGS_PATH']
    cloud_output_dimension_songs_file_name = config['OUTPUT']['CLOUD_OUTPUT_DIMENSION_SONGS_FILE_NAME']
    local_output_dimension_songs_file_name = config['OUTPUT']['LOCAL_OUTPUT_DIMENSION_SONGS_FILE_NAME']

    # Build output environment variables - DIMENSION ARTISTS
    cloud_output_dimension_artists_path = config['OUTPUT']['CLOUD_OUTPUT_DIMENSION_ARTISTS_PATH']
    local_output_dimension_artists_path = config['OUTPUT']['LOCAL_OUTPUT_DIMENSION_ARTISTS_PATH']
    cloud_output_dimension_artists_file_name = config['OUTPUT']['CLOUD_OUTPUT_DIMENSION_ARTISTS_FILE_NAME']
    local_output_dimension_artists_file_name = config['OUTPUT']['LOCAL_OUTPUT_DIMENSION_ARTISTS_FILE_NAME']

    # Build output environment variables - DIMENSION USERS
    cloud_output_dimension_users_path = config['OUTPUT']['CLOUD_OUTPUT_DIMENSION_USERS_PATH']
    local_output_dimension_users_path = config['OUTPUT']['LOCAL_OUTPUT_DIMENSION_USERS_PATH']
    cloud_output_dimension_users_file_name = config['OUTPUT']['CLOUD_OUTPUT_DIMENSION_USERS_FILE_NAME']
    local_output_dimension_users_file_name = config['OUTPUT']['LOCAL_OUTPUT_DIMENSION_USERS_FILE_NAME']

    # Build output environment variables - DIMENSION TIME
    cloud_output_dimension_time_path = config['OUTPUT']['CLOUD_OUTPUT_DIMENSION_TIME_PATH']
    local_output_dimension_time_path = config['OUTPUT']['LOCAL_OUTPUT_DIMENSION_TIME_PATH']
    cloud_output_dimension_time_file_name = config['OUTPUT']['CLOUD_OUTPUT_DIMENSION_TIME_FILE_NAME']
    local_output_dimension_time_file_name = config['OUTPUT']['LOCAL_OUTPUT_DIMENSION_TIME_FILE_NAME']

    # Build output environment variables - FACT SONGPLAYS
    cloud_output_fact_songplays_path = config['OUTPUT']['CLOUD_OUTPUT_FACT_SONGPLAYS_PATH']
    local_output_fact_songplays_path = config['OUTPUT']['LOCAL_OUTPUT_FACT_SONGPLAYS_PATH']
    cloud_output_fact_songplays_file_name = config['OUTPUT']['CLOUD_OUTPUT_FACT_SONGPLAYS_FILE_NAME']
    local_output_fact_songplays_file_name = config['OUTPUT']['LOCAL_OUTPUT_FACT_SONGPLAYS_FILE_NAME']

    # build final environment variables - INPUT DATA
    cloud_input_song_data = os.path.join(cloud_prefix
                                         , cloud_input_data_bucket_name
                                         , cloud_input_song_data_path
                                         , cloud_input_song_data_path_specific
                                         , cloud_input_song_data_file_name
                                         )

    local_input_song_data = os.path.join(local_input_data_path
                                         , local_input_song_data_path
                                         , local_input_song_data_path_specific
                                         , local_input_song_data_file_name
                                         )

    cloud_input_log_data = os.path.join(cloud_prefix
                                        , cloud_input_data_bucket_name
                                        , cloud_input_log_data_path
                                        , cloud_input_log_data_path_specific
                                        , cloud_input_log_data_file_name
                                        )

    local_input_log_data = os.path.join(local_input_data_path
                                        , local_input_log_data_path
                                        , local_input_log_data_path_specific
                                        , local_input_log_data_file_name
                                        )
    # build final environment variables - OUTPUT DATA
    cloud_output_dimension_songs = os.path.join(cloud_prefix
                                                , cloud_output_data_bucket_name
                                                , cloud_output_path
                                                , cloud_output_dimension_songs_path
                                                , cloud_output_dimension_songs_file_name
                                                )

    local_output_dimension_songs = os.path.join(local_output_data_path
                                                , local_output_dimension_songs_path
                                                , local_output_dimension_songs_file_name
                                                )

    cloud_output_dimension_artists = os.path.join(cloud_prefix
                                                  , cloud_output_data_bucket_name
                                                  , cloud_output_path
                                                  , cloud_output_dimension_artists_path
                                                  , cloud_output_dimension_artists_file_name
                                                  )

    local_output_dimension_artists = os.path.join(local_output_data_path
                                                  , local_output_dimension_artists_path
                                                  , local_output_dimension_artists_file_name
                                                  )
    cloud_output_dimension_users = os.path.join(cloud_prefix
                                                , cloud_output_data_bucket_name
                                                , cloud_output_path
                                                , cloud_output_dimension_users_path
                                                , cloud_output_dimension_users_file_name
                                                )
    local_output_dimension_users = os.path.join(local_output_data_path
                                                , local_output_dimension_users_path
                                                , local_output_dimension_users_file_name
                                                )
    cloud_output_dimension_time = os.path.join(cloud_prefix
                                               , cloud_output_data_bucket_name
                                               , cloud_output_path
                                               , cloud_output_dimension_time_path
                                               , cloud_output_dimension_time_file_name
                                               )
    local_output_dimension_time = os.path.join(local_output_data_path
                                               , local_output_dimension_time_path
                                               , local_output_dimension_time_file_name
                                               )
    cloud_output_fact_songplays = os.path.join(cloud_prefix
                                               , cloud_output_data_bucket_name
                                               , cloud_output_path
                                               , cloud_output_fact_songplays_path
                                               , cloud_output_fact_songplays_file_name
                                               )
    local_output_fact_songplays = os.path.join(local_output_data_path
                                               , local_output_fact_songplays_path
                                               , local_output_fact_songplays_file_name
                                               )

    # put prebuild environment variables into dictionary
    return_environment_variables["current_environment"] = "{}".format(current_environment)
    return_environment_variables["cloud_cluster_worker_nodes"] = "{}".format(cloud_cluster_worker_nodes)
    return_environment_variables["local_cluster_worker_nodes"] = "{}".format(local_cluster_worker_nodes)

    # INPUT - DATA
    return_environment_variables["cloud_input_song_data"] = "{}".format(cloud_input_song_data)
    return_environment_variables["local_input_song_data"] = "{}".format(local_input_song_data)
    return_environment_variables["cloud_input_log_data"] = "{}".format(cloud_input_log_data)
    return_environment_variables["local_input_log_data"] = "{}".format(local_input_log_data)

    # OUTPUT - DATA
    return_environment_variables["cloud_output_dimension_songs"] = "{}".format(cloud_output_dimension_songs)
    return_environment_variables["local_output_dimension_songs"] = "{}".format(local_output_dimension_songs)
    return_environment_variables["cloud_output_dimension_artists"] = "{}".format(cloud_output_dimension_artists)
    return_environment_variables["local_output_dimension_artists"] = "{}".format(local_output_dimension_artists)
    return_environment_variables["cloud_output_dimension_users"] = "{}".format(cloud_output_dimension_users)
    return_environment_variables["local_output_dimension_users"] = "{}".format(local_output_dimension_users)
    return_environment_variables["cloud_output_dimension_time"] = "{}".format(cloud_output_dimension_time)
    return_environment_variables["local_output_dimension_time"] = "{}".format(local_output_dimension_time)
    return_environment_variables["cloud_output_fact_songplays"] = "{}".format(cloud_output_fact_songplays)
    return_environment_variables["local_output_fact_songplays"] = "{}".format(local_output_fact_songplays)

    # return the dictionary of environment variables
    return return_environment_variables


def write_parquet_file(df_name, df_to_write, partition_by_parameter, location_to_write, worker_nodes):
    """
    # Write data frame in PARQUET format back to file system
    :param df_name: name of written content
    :param df_to_write: data frame to write into file system
    :param partition_by_parameter: partition columns
    :param location_to_write: where to write file to file system
    :return: none
    """
    # "time_tracker" - timestamp to track running time
    total_execution_start = datetime.datetime.now()
    print("Started write_parquet_file({}): ".format(df_name) + str(total_execution_start))

    # set up worker nodes
    current_worker_nodes = worker_nodes

    df_to_write \
        .repartition(int(current_worker_nodes)) \
        .write \
        .mode(saveMode='Overwrite') \
        .partitionBy(partition_by_parameter) \
        .parquet(location_to_write)

    # "time_tracker"
    total_execution_end = datetime.datetime.now()
    time_tracker("Data_Lake/etl.py/write_parquet_file({})".format(df_name), total_execution_start, total_execution_end)


def process_song_data(spark, environment_variables):
    """
    This procedure reads song metadata in json format and stores it in the Cloud (S3) or Local destination folder
    for analysis
    :param spark: SparkSession - initialized SparkSession object
    :param environment_variables: Dict[Str, Str] - dictionary of all available environment variables
    Return df_songs_with_schema
    """

    # "time_tracker" - timestamp to track running time
    total_execution_start = datetime.datetime.now()
    print("Started process_song_data(): " + str(total_execution_start))

    # get current environment configuration - current_environment = 'LOCAL' | 'CLOUD'
    current_environment = environment_variables.get("current_environment")

    """
     Specification of a StructType for song_data to avoid misinterpretations. For performance reasons this
     specification is also useful.
    """
    schema_song = R([
        Fld("artist_id", Str()),
        Fld("artist_latitude", Dbl()),
        Fld("artist_location", Str()),
        Fld("artist_longitude", Dbl()),
        Fld("artist_name", Str()),
        Fld("duration", Dbl()),
        Fld("num_songs", Int()),
        Fld("song_id", Str()),
        Fld("title", Str()),
        Fld("year", Int())
    ])

    # read song data file (distinguish between LOCAL and CLOUD storage due to the current environment variable)
    df_songs_with_schema = spark \
        .read \
        .schema(schema_song) \
        .json(environment_variables.get("{}_input_song_data"
                                        .format(current_environment).lower()
                                        )
              )

    # Reduce duplicates and extract columns to create d_songs and d_artistis table
    # extract columns to create songs table
    df_d_songs = df_songs_with_schema \
        .select("song_id", "artist_id", "title", "year", "duration") \
        .dropDuplicates() \
        .sort("duration")

    # write songs table to parquet files partitioned by year and artist
    current_cluster_nodes = environment_variables.get("{}_cluster_worker_nodes".format(current_environment).lower())
    partition_by = ["year", "artist_id"]
    location_to_write = environment_variables.get("{}_output_dimension_songs".format(current_environment.lower()))
    write_parquet_file("df_d_songs", df_d_songs, partition_by, location_to_write, current_cluster_nodes)

    # extract columns to create artists table
    df_d_artists = df_songs_with_schema \
        .withColumnRenamed("artist_name", "name") \
        .withColumnRenamed("artist_location", "location") \
        .withColumnRenamed("artist_latitude", "latitude") \
        .withColumnRenamed("artist_longitude", "longitude") \
        .select("artist_id", "name", "location", "latitude", "longitude")

    # write artists table to parquet files
    current_cluster_nodes = environment_variables.get("{}_cluster_worker_nodes".format(current_environment).lower())
    partition_by = []
    location_to_write = environment_variables.get("{}_output_dimension_artists".format(current_environment.lower()))
    write_parquet_file("df_d_artists", df_d_artists, partition_by, location_to_write, current_cluster_nodes)

    # "time_tracker"
    total_execution_end = datetime.datetime.now()
    time_tracker("Data_Lake/etl.py/process_song_data()", total_execution_start, total_execution_end)

    return df_songs_with_schema


def process_log_data(spark, environment_variables):
    """
    This procedure reads log data in json format and stores it in the Cloud (S3) or Local
    destination folder for analysis
    :param spark: SparkSession - initialized SparkSession object
    :param environment_variables: Dict[Str, Str] - dictionary of all available environment variables
    :return: df_log_with_schema
    """
    # "time_tracker" - timestamp to track running time
    total_execution_start = datetime.datetime.now()
    print("Started process_log_data(): " + str(total_execution_start))

    # get current environment configuration - current_environment = 'LOCAL' | 'CLOUD'
    current_environment = environment_variables.get("current_environment")

    """
     Specification of a StructType for log_data to avoid misinterpretations. For performance reasons this
     specification is also useful.
    """
    schema_log = R([
        Fld("artist", Str()),
        Fld("auth", Str()),
        Fld("firstName", Str()),
        Fld("gender", Str()),
        Fld("itemInSession", Lng()),
        Fld("lastName", Str()),
        Fld("length", Dbl()),
        Fld("level", Str()),
        Fld("location", Str()),
        Fld("method", Str()),
        Fld("page", Str()),
        Fld("registration", Dbl()),
        Fld("sessionId", Lng()),
        Fld("song", Str()),
        Fld("status", Lng()),
        Fld("ts", Lng()),
        Fld("userAgent", Str()),
        Fld("userId", Str())
    ])

    # read log data file (distinguish between LOCAL and CLOUD storage due to the current environment variable)
    df_log_with_schema = spark \
        .read \
        .schema(schema_log) \
        .json(environment_variables.get("{}_input_log_data"
                                        .format(current_environment).lower()
                                        )
              )

    # prepare ts (timestamp) for dimension table d_time
    # set valid timestamp format
    ts_format = "yyyy-MM-dd HH:MM:ss z"
    weekday_format = "EEEE"

    # Converting ts to a timestamp format and add column to data frame
    """
    - "withColumn" -- add additional column to df
    - "df_log_table.ts / 1000)" -- cut off milliseconds
    - ".cast(dataType=TimestampType())" -- ts-value (reduced by milliseconds) is casted as TimestampType
    - "to_timestamp("value in Timestampformat", tsFormat)" -- format timestamp into the correct format which 
       is given above
    """
    df_log_with_schema = df_log_with_schema \
        .withColumn('ts_timestamp',
                    to_timestamp(date_format((df_log_with_schema.ts / 1000).cast(dataType=TimestampType()), ts_format)
                                 , ts_format))

    # create time table
    """
    What's going on here?
    - get only the valid timestamp
    - drop duplicate entries. Within the dimension table d_time no duplicates are allowed
    - extract the following things from timestamp: hour, day, week, month, weekday, day_of_week (name of the day)
    - rename columns ts_timestamp, because it's not needed anymore
    """
    df_d_time = df_log_with_schema \
        .select(df_log_with_schema.ts_timestamp) \
        .dropDuplicates() \
        .withColumn("hour", hour(df_log_with_schema.ts_timestamp)) \
        .withColumn("day", dayofmonth(df_log_with_schema.ts_timestamp)) \
        .withColumn("week", weekofyear(df_log_with_schema.ts_timestamp)) \
        .withColumn("month", month(df_log_with_schema.ts_timestamp)) \
        .withColumn("year", year(df_log_with_schema.ts_timestamp)) \
        .withColumn("weekday", dayofweek(df_log_with_schema.ts_timestamp)) \
        .withColumn("day_of_week", date_format(df_log_with_schema.ts_timestamp, weekday_format)) \
        .withColumnRenamed("ts_timestamp", "start_time")

    # write d_time dimension table to parquet files
    current_cluster_nodes = environment_variables.get("{}_cluster_worker_nodes".format(current_environment).lower())
    partition_by = ["year", "month"]
    location_to_write = environment_variables.get("{}_output_dimension_time".format(current_environment.lower()))
    write_parquet_file("df_d_time", df_d_time, partition_by, location_to_write, current_cluster_nodes)

    # Create d_users table
    """
    What's going on here:
    - filter data frame to get only pages="NextSong"
    - get user information with timestamp
    - filter out only the latest user entry
    - write df_d_users table as parquet file
    """
    df_d_users_prep = df_log_with_schema \
        .withColumnRenamed("userId", "user_id") \
        .withColumnRenamed("firstName", "first_name") \
        .withColumnRenamed("lastName", "last_name") \
        .filter(df_log_with_schema.page == "NextSong") \
        .select("user_id", "first_name", "last_name", "gender", "level", "ts_timestamp") \
        .dropDuplicates()

    # get only last entry for each user
    df_d_users_prep.createOrReplaceTempView("users_temp_table")

    df_d_users = spark.sql("""
        select ou.user_id, ou.first_name, ou.last_name, ou.gender, ou.level 
          from users_temp_table as ou
          join (
                select iu.user_id, max(iu.ts_timestamp) as max_timestamp
                  from users_temp_table iu
              group BY iu.user_id
                ) as iu on ou.user_id = iu.user_id and ou.ts_timestamp = iu.max_timestamp
        order by ou.user_id
       """)

    # write d_users dimension table to parquet files
    current_cluster_nodes = environment_variables.get("{}_cluster_worker_nodes".format(current_environment).lower())
    partition_by = []
    location_to_write = environment_variables.get("{}_output_dimension_users".format(current_environment.lower()))
    write_parquet_file("df_d_users", df_d_users, partition_by, location_to_write, current_cluster_nodes)

    # "time_tracker"
    total_execution_end = datetime.datetime.now()
    time_tracker("Data_Lake/etl.py/process_log_data()", total_execution_start, total_execution_end)

    return df_log_with_schema


def process_songplays_table(spark, environment_variables, df_songs, df_log):
    """
    Create table f_songplays:

    - Join the source data frames "df_log" and "df_songs" together by the following columns:
        - df_log.artist = df_songs.artist_name
        - df_log.song = df_songs.title
    - Filter dataset by "df_log.page = 'NextSong'" to reduce loaded data to the important entries for analysis

    :param spark: SparkSession - initialized SparkSession object
    :param environment_variables: Dict[Str, Str] - dictionary of all available environment variables
    :param df_songs: data frame from song files
    :param df_log: data frame from log files
    """

    """
    Extract columns from joined song and log datasets to create songplays table 
    
    The Fact table "f_songplays" can be build in two ways.
    - by SQL Language
    - by using default pyspark methods 
    
    The SQL example is shown ONLY as an example. I'll prefer to use the pyspark methods
    
    --------------------------------------- remove the escape chars (backslash) ----------------------------------------
    # create a tempView for the SQL execution
    df_songs.createOrReplaceTempView("s_staging_songs")
    df_log.createOrReplaceTempView("s_staging_events")
    
    df_f_songplays_sql = spark.sql(\"""
     select distinct
              se.userid as user_id
            , ss.song_id as song_id
            , ss.artist_id as artist_id
            , se.start_time as start_time
            , se.sessionid as session_id
            , se.level as level
            , se.location as location
            , se.useragent as user_agent

            , year(se.start_time) as year
            , month(se.start_time) as month
      from s_staging_events se
      left join s_staging_songs ss on trim(se.artist) = trim(ss.artist_name) and trim(se.song) = trim(ss.title)
     where se.page = 'NextSong'
    ;
    \""")
    --------------------------------------------------------------------------------------------------------------------
    """
    # "time_tracker" - timestamp to track running time
    total_execution_start = datetime.datetime.now()
    print("Started process_songplays_table(): " + str(total_execution_start))

    # get current environment configuration - current_environment = 'LOCAL' | 'CLOUD'
    current_environment = environment_variables.get("current_environment")

    # create an alias, because this is more handy for the later use
    df_s = df_songs.alias('df_s')
    df_l = df_log.alias('df_l')

    # filter data set to pass only "df_l.page == "NextSong" and then join both data sets together by "left join"
    df_f_songplays_joined = df_l \
        .filter(df_l.page == "NextSong") \
        .join(df_s, [(df_l.artist == df_s.artist_name) & (df_l.song == df_s.title)], how='left') \
        .withColumnRenamed("ts_timestamp", "start_time")

    # extract columns from joined song and log datasets to create songplays table
    # songplay_id consists of a window function which is sorted by monotonically_increasing_id()
    df_f_songplays = df_f_songplays_joined \
        .withColumnRenamed("userid", "user_id") \
        .withColumnRenamed("sessionid", "session_id") \
        .withColumnRenamed("useragent", "user_agent") \
        .withColumn("year", year(df_f_songplays_joined.start_time)) \
        .withColumn("month", month(df_f_songplays_joined.start_time)) \
        .withColumn("songplay_id", row_number().over(Window.orderBy(monotonically_increasing_id()))) \
        .select("songplay_id", "user_id", "song_id", "artist_id", "start_time", "session_id", "level", "location",
                "user_agent", "year", "month") \
        .distinct()

    # write songplays table to parquet files partitioned by year and month
    current_cluster_nodes = environment_variables.get("{}_cluster_worker_nodes".format(current_environment).lower())
    partition_by = ["year", "month"]
    location_to_write = environment_variables.get("{}_output_fact_songplays".format(current_environment.lower()))
    write_parquet_file("df_f_songplays", df_f_songplays, partition_by, location_to_write, current_cluster_nodes)

    # "time_tracker"
    total_execution_end = datetime.datetime.now()
    time_tracker("Data_Lake/etl.py/process_songplays_table()", total_execution_start, total_execution_end)


def main():
    """
    Main method of executing etl.py data pipeline to build a five table star Schema
    """

    # "time_tracker" - timestamp to track running time
    total_execution_start = datetime.datetime.now()
    print("Started main(): " + str(total_execution_start))

    # create a new spark session
    spark = create_spark_session()

    """
    set up dict of environment variables
    Arguments   : current_environment = 'LOCAL' | 'CLOUD'
    Return      : return_environment_variables Dict[str, str]
    """
    environment_variables = set_up_environment_variables('LOCAL')
    # environment_variables = set_up_environment_variables('CLOUD')

    # load data from song files and get data frame for later use in process_songplays_data(..)
    df_songs_with_schema = process_song_data(spark, environment_variables)

    # load data from log files
    df_log_with_schema = process_log_data(spark, environment_variables)

    # create fact table f_songplays
    process_songplays_table(spark, environment_variables, df_songs_with_schema, df_log_with_schema)

    # "time_tracker"
    total_execution_end = datetime.datetime.now()
    time_tracker("Data_Lake/etl.py/main()", total_execution_start, total_execution_end)

    # stop the spark, otherwise the program will hang
    spark.stop()


if __name__ == "__main__":
    main()
