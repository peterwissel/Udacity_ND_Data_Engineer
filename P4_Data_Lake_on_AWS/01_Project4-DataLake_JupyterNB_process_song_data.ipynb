{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file in only for developing purposes, because it's not needed to create a Spark Session each time\n",
    "\"\"\"\n",
    "import configparser\n",
    "import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set global variables to access S3 via OS environment variables\n",
    "\"\"\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Spark Session and setting LOG-Level\n",
    "Attributes:\n",
    "    ERROR   - less details - only when something went wrong\n",
    "    WARN    - more details\n",
    "    INFO    - more details than Warn level\n",
    "    DEBUG   - very detailed information\n",
    "\"\"\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"etl pipeline for project 4 - Data Lake\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# setting the current LOG-Level\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://udacity-dend/song_data/A/A/A/TRAAAAK128F9318786.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setting up the path variables\n",
    "- Storage on S3\n",
    "\"\"\"\n",
    "# get filepath to song data file\n",
    "# song_data = 's3a://udacity-dend/song_data/A/A/A/*.json'\n",
    "song_data = 's3a://udacity-dend/song_data/A/A/A/TRAAAAK128F9318786.json'\n",
    "\n",
    "print(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Specification of a StructType for song_data to avoid misinterpretations. For performance reasons this specification is\n",
    "also useful.\n",
    "\n",
    "Personal Info: StructType (Lesson 5 - 6/51) and look at example code (L5 - 26/51 and 28/51)\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.types import StructType as R\\\n",
    "    , StructField as Fld\\\n",
    "    , DoubleType as Dbl\\\n",
    "    , StringType as Str\\\n",
    "    , IntegerType as Int\\\n",
    "    , DateType as Date\n",
    "\n",
    "schema_songs = R([\n",
    "    Fld(\"artist_id\", Str()),\n",
    "    Fld(\"artist_latitude\", Dbl()),\n",
    "    Fld(\"artist_location\", Str()),\n",
    "    Fld(\"artist_longitude\", Dbl()),\n",
    "    Fld(\"artist_name\", Str()),\n",
    "    Fld(\"duration\", Dbl()),\n",
    "    Fld(\"num_songs\", Int()),\n",
    "    Fld(\"song_id\", Str()),\n",
    "    Fld(\"title\", Str()),\n",
    "    Fld(\"year\", Int())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read data with \"new\" schema.\n",
    "\"\"\"\n",
    "df_songWithSchema = spark.read.schema(schema_songs).json(song_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: integer (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "+------------------+---------------+---------------+----------------+------------+--------+---------+------------------+------+----+\n",
      "|artist_id         |artist_latitude|artist_location|artist_longitude|artist_name |duration|num_songs|song_id           |title |year|\n",
      "+------------------+---------------+---------------+----------------+------------+--------+---------+------------------+------+----+\n",
      "|ARJNIUY12298900C91|null           |               |null            |Adelitas Way|213.9424|1        |SOBLFFE12AF72AA5BA|Scream|2009|\n",
      "+------------------+---------------+---------------+----------------+------------+--------+---------+------------------+------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "            artist_id  artist_latitude artist_location  artist_longitude  \\\n0  ARJNIUY12298900C91              NaN                               NaN   \n\n    artist_name  duration  num_songs             song_id   title  year  \n0  Adelitas Way  213.9424          1  SOBLFFE12AF72AA5BA  Scream  2009  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist_id</th>\n      <th>artist_latitude</th>\n      <th>artist_location</th>\n      <th>artist_longitude</th>\n      <th>artist_name</th>\n      <th>duration</th>\n      <th>num_songs</th>\n      <th>song_id</th>\n      <th>title</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ARJNIUY12298900C91</td>\n      <td>NaN</td>\n      <td></td>\n      <td>NaN</td>\n      <td>Adelitas Way</td>\n      <td>213.9424</td>\n      <td>1</td>\n      <td>SOBLFFE12AF72AA5BA</td>\n      <td>Scream</td>\n      <td>2009</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check the new structure and it's data in it\n",
    "\"\"\"\n",
    "print(df_songWithSchema.count())\n",
    "df_songWithSchema.printSchema()\n",
    "df_songWithSchema.show(5, truncate=False)\n",
    "df_songWithSchema.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of lines before: 1\n",
      "Amount of lines after: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "              song_id           artist_id   title  year  duration\n0  SOBLFFE12AF72AA5BA  ARJNIUY12298900C91  Scream  2009  213.9424",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>song_id</th>\n      <th>artist_id</th>\n      <th>title</th>\n      <th>year</th>\n      <th>duration</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SOBLFFE12AF72AA5BA</td>\n      <td>ARJNIUY12298900C91</td>\n      <td>Scream</td>\n      <td>2009</td>\n      <td>213.9424</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Reduce duplicates and extract columns to create d_songs and d_artistis table\n",
    "\"\"\"\n",
    "print(\"Amount of lines before: \" + str(df_songWithSchema.count()) )\n",
    "\n",
    "df_songs_table = df_songWithSchema \\\n",
    "    .select(\"song_id\", \"artist_id\", \"title\", \"year\", \"duration\") \\\n",
    "    .dropDuplicates() \\\n",
    "    .sort(\"duration\")\n",
    "\n",
    "print(\"Amount of lines after: \" + str(df_songWithSchema.count()) )\n",
    "\n",
    "df_songs_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write data frame as JSON or PARQUET back to file system\n",
    "Data to write: d_songs\n",
    ".repartition(1) --> repartition to worker nodes (in a cluster environment you have probably more worker nodes available)\n",
    "\"\"\"\n",
    "df_songs_table \\\n",
    "    .repartition(1) \\\n",
    "    .write \\\n",
    "    .mode(saveMode='Overwrite') \\\n",
    "    .partitionBy(\"year\",\"artist_id\") \\\n",
    "    .parquet('s3a://project-4-data-lake/analytics/d_songs/d_songs.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "            artist_id          name location  latitude  longitude\n0  ARJNIUY12298900C91  Adelitas Way                NaN        NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist_id</th>\n      <th>name</th>\n      <th>location</th>\n      <th>latitude</th>\n      <th>longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ARJNIUY12298900C91</td>\n      <td>Adelitas Way</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_artistis_table = df_songWithSchema \\\n",
    "    .withColumnRenamed(\"artist_name\", \"name\") \\\n",
    "    .withColumnRenamed(\"artist_location\", \"location\") \\\n",
    "    .withColumnRenamed(\"artist_latitude\", \"latitude\") \\\n",
    "    .withColumnRenamed(\"artist_longitude\", \"longitude\") \\\n",
    "    .select(\"artist_id\", \"name\", \"location\", \"latitude\", \"longitude\")\n",
    "\n",
    "df_artistis_table.limit(5).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write data frame as JSON or PARQUET back to file system\n",
    "Data to write: d_artists\n",
    "- possible write modes: \"Overwrite\" and \"Append\"\n",
    "\"\"\"\n",
    "df_artistis_table \\\n",
    "    .repartition(1) \\\n",
    "    .write \\\n",
    "    .mode(saveMode='Overwrite') \\\n",
    "    .parquet('s3a://project-4-data-lake/analytics/d_artists/d_artists.parquet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nSpark Read and Write Apache Parquet file\\nhttps://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/\\n'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Write data frame \"df_songWithSchema\" as PARQUET file to file system for later use in file\n",
    "\"Project4-DataLake_JupyterNB_create_f_songplays_table.ipynb\". This is only temp data!!\n",
    "\n",
    "Data to write: df_songWithSchema\n",
    "- possible write modes: \"Overwrite\" and \"Append\"\n",
    "\"\"\"\n",
    "df_songWithSchema \\\n",
    "    .repartition(1) \\\n",
    "    .write \\\n",
    "    .mode(saveMode='Overwrite') \\\n",
    "    .parquet('s3a://project-4-data-lake/analytics/staging/df_songs_table.parquet')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Spark Read and Write Apache Parquet file\n",
    "https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}