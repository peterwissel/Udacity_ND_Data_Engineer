{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project 08 - Analysis of U.S. Immigration (I-94) Data\n",
    "### Udacity Data Engineer - Capstone Project\n",
    "> by Peter Wissel | 2021-04-03\n",
    "\n",
    "## Project Overview\n",
    "This project works with a data set for immigration to the United States. The supplementary datasets will include data on\n",
    "airport codes, U.S. city demographics and temperature data.\n",
    "\n",
    "The following process is divided into five sub-steps to illustrate how to answer the questions set by the business\n",
    "analytics team.\n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "##### Scope of the Project\n",
    "Based on the given data set, the following four project questions (PQ) are posed for business analysis, which need to be\n",
    " answered in this project. The data pipeline and star data model are completely aligned with the questions.\n",
    "\n",
    "1. From which country do immigrants come to the U.S. and how many?\n",
    "2. At what airports do foreign persons arrive for immigration to the U.S.?\n",
    "3. At what times do foreign persons arrive for immigration to the U.S.?\n",
    "4. To which states in the U.S. do immigrants want to continue their travel after their initial arrival and what\n",
    "   demographics can immigrants expect when they arrive in the destination state, such as average temperature, population\n",
    "   numbers or population density?\n",
    "\n",
    "\n",
    "##### Gather Data\n",
    "The project works primarily with a dataset based on immigration data (I94) to the United States.\n",
    "\n",
    "- Gathering Data (given data sets):\n",
    "    1. [Immigration data '18-83510-I94-Data-2016' to the U.S.](https://travel.trade.gov/research/programs/i94/description.asp)\n",
    "    2. [airport-codes_csv.csv: Airports around the world](https://datahub.io/core/airport-codes#data)\n",
    "    3. [us-cities-demographics.csv: US cities and it's information about citizens](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "    4. [GlobalLandTemperaturesByCity.csv: Temperature grouped by City and Country](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "The next step is used to find insights within given data.\n",
    "\n",
    "#### Summary for Immigration data `18-83510-I94-Data-2016` to the U.S.:\n",
    "* **Source**: [Visitor Arrivals Program (I-94 Form)](https://travel.trade.gov/research/programs/i94/description.asp)\n",
    "* **Description**: [I94_SAS_Labels_Descriptions.SAS](../P8_capstone_resource_files/I94_SAS_Labels_Descriptions.SAS) file\n",
    "contains descriptions for the I94 data\n",
    "* **Data**: Month based dataset for year 2016\n",
    "* **Format**: SAS (SAS7BDAT - e.g. `i94_apr16_sub.sas7bdat`)\n",
    "* **Rows**: Over 3 million lines for each file. In total, about 40 million lines.\n",
    "* **Data description**: Data has 29 columns containing information about event date, arriving person, airport, airline, etc.\n",
    "![I94-immigration-data example](../P8_capstone_documentation/10_P8_immigration_data_sample.png)\n",
    "NOTE: The Data has to be paid. Year 2016 is included and available for Udacity DEND course.\n",
    "\n",
    "##### Immigration data '18-83510-I94-Data-2016' to the U.S.\n",
    "   The descriptions for the listed columns were taken from file [I94_SAS_Labels_Descriptions.SAS](../P8_capstone_resource_files/I94_SAS_Labels_Descriptions.SAS).\n",
    "\n",
    "    - **i94yr:** 4 digit year\n",
    "    - **i94mon:** numeric month\n",
    "    - **i94cit + i94res:** Country where the immigrants come from - `Country code, country name`\n",
    "    Look at file [I94_SAS_Labels_I94CIT_I94RES.txt](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94CIT_I94RES.txt) for more details.\n",
    "\n",
    "            438 =  'AUSTRALIA'\n",
    "            112 =  'GERMANY'\n",
    "    ! Note that the I94 country codes are different from the ISO country numbers.\n",
    "\n",
    "   - **i94port:** arrival airport - `Airport code, Airport city, State of Airport`. Note that the airport code is **not** the same as the [IATA](https://en.wikipedia.org/wiki/International_Air_Transport_Association) code.\n",
    "     [IATA-Code Search Engine](https://www.iatacodes.de/)\n",
    "\n",
    "   The data of the I-94 table do not correspond to the current ISO standards. Therefore, `SFR` is used for San\n",
    "   Francisco Airport rather than the more common `SFO` designation.\n",
    "\n",
    "            'SFR'\t=\t'SAN FRANCISCO, CA     '\n",
    "            'LOS'\t=\t'LOS ANGELES, CA       '\n",
    "            'NYC'\t=\t'NEW YORK, NY          '\n",
    "\n",
    "    Look at file [I94_SAS_Labels_I94PORT.txt](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94PORT.txt) for more details.\n",
    "\n",
    "   - **arrdate:** Arrival date in the U.S. (SAS Date format)\n",
    "\n",
    "            SAS: Start Date is 01.01.1960 (SAS - Days since 1/1/1960: 0)\n",
    "            Example:\n",
    "            01.01.1960: (SAS: Days since 1/1/1960: 0)\n",
    "            01.01.1970: (SAS: Days since 1/1/1960: 3653)\n",
    "\n",
    "        Take a look at [Free SAS Date Calculator](https://www.sastipsbyhal.com)\n",
    "\n",
    "\n",
    "    - **i94mode:** Type of immigration to U.S.\n",
    "    Look at file [I94_SAS_Labels_I94MODE.txt](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94MODE.txt) for more details.\n",
    "\n",
    "            1 = 'Air'\n",
    "            2 = 'Sea'\n",
    "            3 = 'Land'\n",
    "            9 = 'Not reported'\n",
    "\n",
    "    - **i94addr:** Location State where the immigrants want travel to.\n",
    "      Look at file [I94_SAS_Labels_I94ADDR.txt](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94ADDR.txt) for more details.\n",
    "\n",
    "            'AL'='ALABAMA'\n",
    "            'IN'='INDIANA'\n",
    "\n",
    "    - **depdate:** Departure date from USA (SAS Date format) -> look at `arrdate` for calculation\n",
    "\n",
    "    - **i94bir:** Age of respondent in years\n",
    "    - **i94ivsa:** Visa codes collapsed into three categories:\n",
    "      Look at file [I94_SAS_Labels_I94VISA.txt](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94VISA.txt) for more details.\n",
    "\n",
    "            1 = Business\n",
    "            2 = Pleasure\n",
    "            3 = Student\n",
    "\n",
    "    - **count:** value is for summary statistics\n",
    "    - **dtadfile:** Date added to I-94 Files - Character date field as YYYYMMDD (represents `arrdate`)\n",
    "    - **visapost:** Department of state where Visa was issued\n",
    "    - **occup:** Occupation that will be performed in U.S.\n",
    "    - **entdepa:** Arrival Flag - admitted or paroled into the U.S.\n",
    "    - **entdepd:** Departure Flag - Departed, lost I-94 or is deceased\n",
    "    - **entdepu:** Update Flag - Either apprehended, overstayed, adjusted to perm residence\n",
    "    - **matflag:** Match flag - Match of arrival and departure records\n",
    "    - **biryear:** 4 digit year of birth\n",
    "    - **dtaddto:** Date to which admitted to U.S. (allowed to stay until) - Character date field as MMDDYYYY (represents `depdate`)\n",
    "    - **gender:** Gender - Non-immigrant sex\n",
    "    - **insnum:** Insurance (INS) number\n",
    "    - **airline:** Airline used to arrive in U.S.\n",
    "    - **admnum:** Admission Number\n",
    "    - **fltno:** Flight number of Airline used to arrive in U.S.\n",
    "    - **viatype:** Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imports and Installs section"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "# import spark as spark\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType, LongType, TimestampType, DateType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession, DataFrameNaFunctions\n",
    "from pyspark.sql.functions import when, count, col, to_date, datediff, date_format, month\n",
    "import re\n",
    "import json\n",
    "from os import path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Create Pandas and SparkSession to create data frames from source data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If code will be executed in Udacity workbench --> use the following config(...)\n",
    "#spark = SparkSession.builder.config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# The version number for \"saurfang:spark-sas7bdat\" had to be updated for the local installation\n",
    "MAX_MEMORY = \"5g\"\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"etl pipeline for project 8 - I94 data\") \\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:3.0.0-s_2.12\")\\\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .appName(\"Foo\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# setting the current LOG-Level\n",
    "spark.sparkContext.setLogLevel('ERROR')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read data from Immigration data '18-83510-I94-Data-2016' to the U.S.\n",
    "filepath = '../P8_capstone_resource_files/immigration_data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat'\n",
    "df_pd_i94 = pd.read_sas(filepath, format=None, index=None, encoding=None, chunksize=None, iterator=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show data (1st 5 rows)\n",
    "df_pd_i94.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show data (last 5 rows)\n",
    "df_pd_i94.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get an overview about filled fields (not null)\n",
    "df_pd_i94.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Summary for Airport Codes [`airport-codes_csv.csv`](../P8_capstone_resource_files/airport-codes_csv.csv):\n",
    "* **Source**: [datahub.io - Airport codes](https://datahub.io/core/airport-codes#data)\n",
    "* **Description**: Airport codes from around the world contain codes that may refer to either IATA airport code, a\n",
    "  three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport\n",
    "  code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code.\n",
    "* **Data**: Large file, containing information about all airports from [this site](https://ourairports.com/data/)\n",
    "* **Format**: CSV File - Comma separated text file format\n",
    "* **Rows**: over 55k\n",
    "* **Data description**: Detailed information about each listed airport is displayed in 12 columns.\n",
    "  ![08_P8_airport-codes_csv.png](../P8_capstone_documentation/08_P8_airport-codes_csv.png)\n",
    "\n",
    "\n",
    "##### Read data from file Airport Codes: `airport-codes_csv.csv`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filepath = '../P8_capstone_resource_files/airport-codes_csv.csv'\n",
    "df_pd_airport = pd.read_csv(filepath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show data (1st 5 rows)\n",
    "df_pd_airport.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show data (last 5 rows)\n",
    "df_pd_airport.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get an overview about filled fields\n",
    "df_pd_airport.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Summary for US Cities: Demographics [`us-cities-demographics.json`](../P8_capstone_resource_files/us-cities-demographics.json):\n",
    "* **Source:** [US Cities: Demographics ](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/)\n",
    "* **Description:** This dataset contains information about the demographics of all US cities and census-designated places\n",
    "  with a population greater or equal to 65,000. This data comes from the [US Census Bureau's 2015 American Community Survey](https://www.census.gov/en.html).\n",
    "* **Data:** Structured data about City, State, Age, Population, etc.\n",
    "* **Format:** JSON File - Structured data\n",
    "* **Rows:** 2,8k\n",
    "* **Data description:** 12 columns describing facts from cities across the U.S. about demographics.\n",
    "  ![15_P8_us-cities-demographics.png](../P8_capstone_documentation/12_P8_us-cities-demographics.png)\n",
    "\n",
    "\n",
    "##### Read data from file US Cities and it's information about citizens: `us-cities-demographics.csv:`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filepath = '../P8_capstone_resource_files/us-cities-demographics.json'\n",
    "df_pd_us_cities = pd.read_json(filepath, orient='columns')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show data (1st 5 rows)\n",
    "df_pd_us_cities.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show data (last 5 rows)\n",
    "df_pd_us_cities.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get an overview about filled fields\n",
    "df_pd_us_cities.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Summary for World Temperature Data [`GlobalLandTemperaturesByCity.csv`](../P8_capstone_resource_files/GlobalLandTemperaturesByCity.csv):\n",
    "* **Source:** [World Temperature Data: Temperature grouped by City and Country](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "* **Description:** Climate Change: Earth Surface Temperature Data. Global temperatures since 1750.\n",
    "* **Data:**  Structured data about Average Temperature, City, Country, Location (Latitude and Longitude)\n",
    "* **Format:** CSV File - Comma separated text file format\n",
    "* **Rows:** 8,5 million entries\n",
    "* **Data description:** Temperature record as time series information since 1750.\n",
    "  ![09_P8_GlobalLandTemperaturesByCity.png](../P8_capstone_documentation/09_P8_GlobalLandTemperaturesByCity.png)\n",
    "* **Note:** Temperature data must be formatted correctly\n",
    "\n",
    "##### Read data from World Temperature Data where Temperature is grouped by City and Country: `GlobalLandTemperaturesByCity.csv`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filepath = '../P8_capstone_resource_files/GlobalLandTemperaturesByCity.csv'\n",
    "df_pd_temperature = pd.read_csv(filepath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show data (1st 5 rows)\n",
    "df_pd_temperature.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show data (last 5 rows)\n",
    "df_pd_temperature.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get an overview about filled fields\n",
    "df_pd_temperature.count()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Findings from Immigration data `18-83510-I94-Data-2016` to the U.S.:\n",
    "\n",
    "###### 1. `df_spark_i94.i94cit`:\n",
    "- County Code does not match to `iso-3166`-Country-Code for further analysis\n",
    "- Null values in column `i94cit`\n",
    "\n",
    "###### 2. `df_spark_i94.i94port`:\n",
    "- Airport Code `i94port` does not correspondent to [IATA](https://en.wikipedia.org/wiki/International_Air_Transport_Association)\n",
    "3 letter airport codes from file [I94_SAS_Labels_I94PORT.txt](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94PORT.txt).\n",
    "**Project decision**: Only usage of given i94 airport codes.\n",
    "\n",
    "###### 3. `df_spark_i94.arrdate` / `df_spark_i94.depdate`:\n",
    "- `arrdate` and `depdate` are in SAS date format (String), whose epoch starts on 1960-01-01. This date values will be converted into DateFormat.\n",
    "\n",
    "###### 4. `df_spark_i94.i94addr`:\n",
    "- Null values in column `i94addr`\n",
    "\n",
    "###### 5. [I94_SAS_Labels_I94ADDR.txt.I94ADDR](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94ADDR.txt):\n",
    "- `I94ADDR` State description has errors like 'WI'='WISCONS**O**N' instead of 'WI'='WISCONS**I**N'. **Project decision:**\n",
    "The only incorrect US state will be corrected manually.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3.0: Define the Data Model\n",
    "#### Conceptual Data Model\n",
    "The following data model is based on the four main questions to be answered. For this reason, I decided to select only\n",
    "the fields from the source data that provide the correct answers. After the data has been read in and written to the\n",
    "staging tables for transformation, a star data model will be created for data analysis. Note: This project is not large enough\n",
    "to store the data in a core data warehouse (3NR) as a preliminary stage.\n",
    "\n",
    "![I94-Data ER-Model](../P8_capstone_documentation/05_P8_I94-Data_Staging_Tables.png)\n",
    "![I94-Data Staging Tables](../P8_capstone_documentation/06_P8_I94-Data_Model.png)\n",
    "\n",
    "#### Mapping Out Data Pipelines\n",
    "Here is the list of steps to pipeline the data into the chosen data model to answer the project questions.\n",
    "\n",
    "**Project decision:** As already mentioned the data model is built up step by step always the project questions in mind.\n",
    "Another common way to build the star data model is to create all staging tables and then the dimension and fact tables.\n",
    "This is not part of the following description.\n",
    "\n",
    "\n",
    "##### 3.1.1. From which country do immigrants come to the U.S. and how many? [(Data pipeline)](#question1_data_pipeline) <a name=\"question1_description\">\n",
    "1. Clean data and create staging table `st_i94_immigration` from files `i94_<month>16_sub.sas7bdat`\n",
    "2. Clean data and create staging table `st_immigration_countries` from file\n",
    "   [`I94_SAS_Labels_I94CIT_I94RES.txt`](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94CIT_I94RES.txt)\n",
    "3. Creation of a fact table named `f_i94_immigration` based on staging table `st_i94_immigration`.\n",
    "4. Creation of a dimension named `d_immigration_countries` based on staging table `st_immigration_countries`.\n",
    "5. Mapping of dimension `d_immigration_countries` to  fact table `f_i94_immigration` based on columns\n",
    "   (`st_i94_immigration.st_i94_cit` --> `f_i94_immigration.d_ic_id`) == (`st_immigration_countries.st_ic_country_code`\n",
    "   --> `d_immigration_countries.d_ic_id` )\n",
    "6. Answer Project Question 1: From which country do immigrants come to the U.S. and how many?\n",
    "\n",
    "**NOTE:** The three columns `st_i94_port_iso`, `st_i94_port_state_code` and `st_i94_port_city` will be inserted after\n",
    "creation of the staging table `st_immigration_airports` within the next step.\n",
    "\n",
    "##### 3.1.2. At what airports do foreign persons arrive for immigration to the U.S.? [(Data pipeline)](#question2_data_pipeline) <a name=\"question2_description\">\n",
    "**Airport dimension**\n",
    "1. Clean data and create staging table `st_immigration_airports` from file\n",
    "   [`I94_SAS_Labels_I94PORT.txt`](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94PORT.txt)\n",
    "   with the columns `st_ia_airport_code` as referencing column, `st_ia_airport_name` and `st_ia_airport_state_code`.\n",
    "\n",
    "    Note that the I-94 airport code is **not** the same as the [IATA](https://en.wikipedia.org/wiki/International_Air_Transport_Association) code and\n",
    "    does not correspond to it. Therefore, `SFR` (I94: 'SFR' = 'SAN FRANCISCO, CA') is used for San\n",
    "    Francisco Airport in this scenario instead of `SFO`. `SFR` means normally San Fernando, CA, USA.\n",
    "\n",
    "    **Project decision:** Data from file [airport-codes.csv](../P8_capstone_resource_files/airport-codes_csv.csv) will **not** be linked to the\n",
    "    I-94 airport codes because incorrect assignments should not be made.\n",
    "2. Add the column `st_i94_port_state_code` to staging table `st_i94_immigration` based on staging table `st_immigration_airports`. This\n",
    "   information is needed to connect the `us-cities-demographics.json` file later on.\n",
    "   `st_ia_airport_state_code --> st_i94_port_state_code`\n",
    "3. Add column `st_i94_port_state_code --> f_i94_port_state_code` to fact table `f_i94_immigrations`\n",
    "4. Creation of a dimension named `d_immigration_airports` based on staging table `st_immigration_airports`.\n",
    "5. Mapping of dimension `d_immigration_airports` to  fact table `f_i94_immigration` based on columns\n",
    "   (`st_immigration_airports.st_ia_airport_code` --> `d_immigration_airports.d_ia_id`) ==\n",
    "   (`st_i94_immigration.st_i94_port` --> `f_i94_immigration.d_ia_id`).\n",
    "6. Answer Project Question 2: At what airports do foreign persons arrive for immigration to the U.S.?\n",
    "\n",
    "##### 3.1.3. At what times do foreign persons arrive for immigration to the U.S.? [(Data pipeline)](#question3_data_pipeline) <a name=\"question3_description\">\n",
    "**Date dimensions**\n",
    "\n",
    "`st_i94_arrdate` and `st_i94_depdate` from staging table `st_i94_immigration` describe dates in SAS specific Date format.\n",
    "The SAS date calculation starts on 1960-01-01. These columns are converted to DateType format in the staging table\n",
    "`st_i94_immigrations` as columns named `st_i94_arrdate_iso` and `st_i94_arrdate_iso`.\n",
    "\n",
    "Get date values from columns `st_i94_immigration.st_i94_arrdate_iso` and `st_i94_immigration.st_i94_depdate_iso`.\n",
    "Get a valid MIN(), MAX() and default (null value representation) date. Clean data and rewrite staging table 'st_i94_immigrations' if needed.\n",
    "Finally, create two dimensions 'd_date_arrivals' and 'd_date_departures' out of it without gaps.\n",
    "\n",
    "1. Read data and get min() and max() value out of `st_i94_arrdate_iso` and `st_i94_depdate_iso`\n",
    "2. Clean date column \"st_i94_depdate_iso\": Valid entries are between 2016-01-01 and 2017-06-14. Pre- and descending values\n",
    "   will be set to null / default value (1900-01-01)\n",
    "3. Update fact table `f_i94_immigrations` based on cleaned column `st_i94_depdate_iso`  values inside\n",
    "4. Generate new date staging tables (`st_date_arrivals`, `st_date_departures`) based on default, min and max values\n",
    "5. Append date specific columns to staging tables, create a dimension out of it and store it\n",
    "6. Map dimension `d_date_arrivals` to  fact table `f_i94_immigration` based on columns\n",
    "   (`st_date_arrivals.st_da_date` --> `d_date_arrivals.d_da_id`) == (`st_i94_immigration.st_i94_arrdate_iso` --> `f_i94_immigration.d_da_id`).\n",
    "7. Map dimension `d_date_departures` to  fact table `f_i94_immigration` based on columns\n",
    "   (`st_date_departures.st_dd_date` --> `d_date_departures.d_dd_id`) == (`st_i94_immigration.st_i94_depdate_iso` --> `f_i94_immigration.d_dd_id`).\n",
    "8. Answer Project Question 3.1: At what times do foreign persons arrive for immigration to the U.S.?\n",
    "9. Answer Project Question 3.2: When a foreign person comes to the U.S. for immigration, do they travel on to another state?\n",
    "10. Answer Project Question 3.3: If a foreign person travels to another state, after which period of time does this happen?\n",
    "\n",
    "\n",
    "The creation of those two date dimensions is based on one physical table. This method is called\n",
    "[Role-Playing Dimensions](https://dba.stackexchange.com/questions/137971/how-many-date-dimensions-for-one-fact)\n",
    "![Role-Playing Dimension](../P8_capstone_documentation/11_P8_RolePlayingDimension.png).\n",
    "\n",
    "\n",
    "##### 3.1.4. To which states in the U.S. do immigrants want to continue their travel after their initial arrival and what demographics can immigrants expect when they arrive in the destination state, such as average temperature, population numbers or population density? [(Data pipeline)](#question4_data_pipeline) <a name=\"question4_description\">\n",
    "1. Clean data and create staging table `st_state_destinations` from file\n",
    "   [I94_SAS_Labels_I94ADDR.txt](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94ADDR.txt)\n",
    "   based on columns `st_sd_state_code` and `st_sd_state_name`.\n",
    "2. Extract some demographic data from file [us-cities-demographics.json](../P8_capstone_resource_files/us-cities-demographics.json)\n",
    "   like `age_median`, `population_male`, `population_female`, `population_total` or `foreign_born` and add them to staging\n",
    "   table `st_state_destinations`.\n",
    "3. Creation of a dimension named `d_state_destinations` based on staging table `st_state_destinations`.\n",
    "4. Mapping of dimension `d_state_destinations` to  fact table `f_i94_immigration` based on columns\n",
    "   (`st_state_destinations.st_sd_state_code` --> `d_state_destinations.d_sd_id`) ==\n",
    "   (`st_i94_immigration.st_i94_addr` --> `f_i94_immigration.d_sd_id`).\n",
    "5. Clean fact table `f_i94_immigration` based on the dimension `d_state_destinations`. All unrecognizable columns will\n",
    "   be set to 99 (all other countries).\n",
    "6. Answer Project Question 4: To which states in the U.S. do immigrants want to continue their travel after their initial\n",
    "   arrival and what demographics can immigrants expect when they arrive in the destination state, such as average\n",
    "   temperature, population numbers or population density?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4.0: Run ETL to Model the Data\n",
    "The following steps describe the ETL data pipeline.\n",
    "\n",
    "#### 4.1 Create the data model\n",
    "The data pipelines are built to create the data model.\n",
    "\n",
    "##### 4.1.1. From which country do immigrants come to the U.S. and how many? [(Description)](#question1_description) <a name=\"question1_data_pipeline\">\n",
    "\n",
    "1. Clean data and create staging table `st_i94_immigration` from files `i94_<month>16_sub.sas7bdat`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Convert SAS data into spark parquet files as 1st staging step #####"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# original path in Udacity workspace\n",
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark')\n",
    "# .load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\n",
    "# The SAS files (e.g. i94_apr16_sub.sas7bdat) are partitioned by month. The for loop extracts each file and stores it\n",
    "# partitioned by month in parquet format.\n",
    "\n",
    "months_abbreviation = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "\n",
    "for current_month in months_abbreviation:\n",
    "    month_abbreviation = current_month\n",
    "\n",
    "    filepath_i94 = f\"../P8_capstone_resource_files/immigration_data/18-83510-I94-Data-2016/\" \\\n",
    "                   f\"i94_{month_abbreviation}16_sub.sas7bdat\"\n",
    "    print(filepath_i94)\n",
    "\n",
    "    # load current month\n",
    "    df_spark_i94 = spark\\\n",
    "        .read\\\n",
    "        .format('com.github.saurfang.sas.spark')\\\n",
    "        .load(filepath_i94)\n",
    "\n",
    "    \"\"\"\n",
    "    Note: optionally load conditions:\n",
    "            .load(filepath_i94,\n",
    "                  forceLowercaseNames=True,\n",
    "                  inferLong=True)\n",
    "    \"\"\"\n",
    "\n",
    "    # write data and append all month to the same parquet result set\n",
    "    location_to_write = \"../P8_capstone_resource_files/parquet_raw/i94_sas_data\"\n",
    "\n",
    "    # delete folder if already exists\n",
    "    if path.exists(location_to_write):\n",
    "        shutil.rmtree(location_to_write)\n",
    "\n",
    "    # write data frame as parquet file (ca. 815 MB)\n",
    "    df_spark_i94 \\\n",
    "        .repartition(int(1)) \\\n",
    "        .write \\\n",
    "        .mode(saveMode='append') \\\n",
    "        .partitionBy('i94mon') \\\n",
    "        .parquet(location_to_write, compression=\"gzip\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Optional write methods (.csv & .csv.gz)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    location_to_write = \"../P8_capstone_resource_files/parquet_raw/i94_data.csv\"\n",
    "\n",
    "    # delete folder if already exists\n",
    "    if path.exists(location_to_write):\n",
    "        shutil.rmtree(location_to_write)\n",
    "\n",
    "    # write data frame as uncompressed CSV file (approx. 5,9 GB)\n",
    "    df_spark_i94\\\n",
    "        .coalesce(1)\\\n",
    "        .write\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .csv(location_to_write)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # delete folder if already exists\n",
    "    if path.exists(location_to_write):\n",
    "        shutil.rmtree(location_to_write)\n",
    "\n",
    "    # write data frame as compressed CSV file (approx. 885 MB)\n",
    "    df_spark_i94\\\n",
    "        .coalesce(1)\\\n",
    "        .write\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"codec\", \"org.apache.hadoop.io.compress.GzipCodec\")\\\n",
    "        .csv(\"../P8_capstone_resource_files/parquet_raw/i94_data.csv.gz\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Check written data frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read written data frame back into memory\n",
    "df_spark_i94 = spark.read.parquet(\"../P8_capstone_resource_files/parquet_raw/i94_sas_data\")\n",
    "\n",
    "# read only three month of data\n",
    "#df_spark_i94 = spark.read.parquet(\"../P8_capstone_resource_files/parquet/i94_sas_data/i94mon=12.0\")\n",
    "\"\"\"\n",
    "df_spark_i94 = spark.read\\\n",
    "    .parquet(\"../P8_capstone_resource_files/parquet_raw/i94_sas_data/i94mon=4.0\",\n",
    "             \"../P8_capstone_resource_files/parquet_raw/i94_sas_data/i94mon=5.0\",\n",
    "             \"../P8_capstone_resource_files/parquet_raw/i94_sas_data/i94mon=6.0\")\n",
    "\n",
    "\n",
    "df_spark_i94 = spark.read\\\n",
    "    .parquet(\"../P8_capstone_resource_files/parquet_raw/i94_sas_data/i94mon=1.0\",\n",
    "             \"../P8_capstone_resource_files/parquet_raw/i94_sas_data/i94mon=2.0\",\n",
    "             \"../P8_capstone_resource_files/parquet_raw/i94_sas_data/i94mon=3.0\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get lines of data from data frame\n",
    "df_spark_i94.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check current df Schema\n",
    "df_spark_i94.printSchema()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show Summary statistics. Attention: This could take very long to compute!\n",
    "df_spark_i94.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if the Conversion step worked as expected\n",
    "### !!!!!!! Keep in mind: Setup virtual environment path: ..../Project_8_Data_Engineering_Capstone_Project/venv/bin/python3  because we use UDF --> python code !!!!!!!!!!!!!!!!!!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Preparation to get an enumeration of all elements within the data frame and a\n",
    "# UDF to convert a SAS date (Integer Format) into a DateType() format.\n",
    "\n",
    "from pyspark.sql.functions import row_number,lit\n",
    "from pyspark.sql.window import Window\n",
    "w = Window().orderBy(lit('A'))\n",
    "\n",
    "# register UDF function to calculate a DateType from given SAS date format\n",
    "getDateFomSASDate = F.udf(lambda y: get_date_from_sas_date(y), DateType())\n",
    "spark.udf.register(\"getDateFomSASDate\", getDateFomSASDate)\n",
    "\n",
    "# Function to convert a SAS date into a DateType\n",
    "\"\"\"\n",
    "Convert SAS date into a DateType value. If sas_date == 0 then choose the default value 1960-01-01.\n",
    "\"\"\"\n",
    "def get_date_from_sas_date(sas_date):\n",
    "    sas_date_int = int(sas_date)\n",
    "    if sas_date_int > 0:\n",
    "        return datetime(1960, 1, 1) + timedelta(days=sas_date_int)\n",
    "    else:\n",
    "        return datetime(1900, 1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Transformation of the originally stored data from files `i94_<month>16_sub.sas7bdat`\n",
    "# read parquet file\n",
    "# fill up null values\n",
    "# convert data into new columns\n",
    "# select only needed columns\n",
    "\n",
    "df_st_i94_immigrations = spark\\\n",
    "    .read\\\n",
    "    .parquet(\"../P8_capstone_resource_files/parquet_raw/i94_sas_data\")\\\n",
    "    .fillna(value=0.0 ,subset=['i94cit'])\\\n",
    "    .fillna(value='99', subset=['i94addr'])\\\n",
    "    .fillna(value=0.0, subset=['depdate'])\\\n",
    "    .fillna(value='99991231', subset=['dtadfile'])\\\n",
    "    .fillna(value='NA', subset=['matflag'])\\\n",
    "    .withColumn(\"st_i94_cit\", F.round(\"i94cit\", 0).cast(IntegerType()))\\\n",
    "    .withColumn(\"st_i94_port\", col(\"i94port\"))\\\n",
    "    .withColumn(\"st_i94_addr\", col(\"i94addr\"))\\\n",
    "    .withColumn(\"st_i94_arrdate\", F.round(\"arrdate\").cast(IntegerType()))\\\n",
    "    .withColumn(\"st_i94_arrdate_iso\", getDateFomSASDate(\"arrdate\"))\\\n",
    "    .withColumn(\"st_i94_depdate\", F.round(\"depdate\").cast(IntegerType()))\\\n",
    "    .withColumn(\"st_i94_depdate_iso\", getDateFomSASDate(\"depdate\"))\\\n",
    "    .withColumn('st_i94_dtadfile', to_date('dtadfile','yyyyMMdd')) \\\n",
    "    .withColumn(\"st_i94_matflag\", col(\"matflag\"))\\\n",
    "    .withColumn(\"st_i94_count\", F.round(\"count\", 0).cast(IntegerType()))\\\n",
    "    .withColumn(\"st_i94_year\", col(\"i94yr\").cast(IntegerType()))\\\n",
    "    .withColumn(\"st_i94_month\", col(\"i94mon\").cast(IntegerType())) \\\n",
    "    .select(\n",
    "              \"st_i94_cit\",\n",
    "              \"st_i94_port\",\n",
    "              \"st_i94_addr\",\n",
    "              \"st_i94_arrdate\",\n",
    "              \"st_i94_arrdate_iso\",\n",
    "              \"st_i94_depdate\",\n",
    "              \"st_i94_depdate_iso\",\n",
    "              \"st_i94_dtadfile\",\n",
    "              \"st_i94_matflag\",\n",
    "              \"st_i94_count\",\n",
    "              \"st_i94_year\",\n",
    "              \"st_i94_month\" )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compare the counts of the full dataset\n",
    "# Count of rows         : 40.790.529\n",
    "# Count of distinct rows: 12.228.839\n",
    "print('Count of rows: {0}'.format(df_st_i94_immigrations.count()))\n",
    "print('Count of distinct rows: {0}'.format(df_st_i94_immigrations.distinct().count()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# clean up complete identical rows. Only do this if the results from the step above are not identical!\n",
    "df_st_i94_immigrations = df_st_i94_immigrations.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compare the counts of the full dataset again\n",
    "# Count of rows         : 12.228.839\n",
    "# Count of distinct rows: 12.228.839\n",
    "print('Count of rows: {0}'.format(df_st_i94_immigrations.count()))\n",
    "print('Count of distinct rows: {0}'.format(df_st_i94_immigrations.distinct().count()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# After dropping duplicates we create for each row a unique ID.\n",
    "# The F.row_number().over(w)) method gives each record a unique and increasing ID and starts with 1.\n",
    "# The F.monotonicallymonotonically_increasing_id() method gives each record a unique and increasing ID and starts with 0.\n",
    "df_st_i94_immigrations = df_st_i94_immigrations\\\n",
    "    .sort(\"st_i94_year\", \"st_i94_month\", \"st_i94_cit\") \\\n",
    "    .withColumn(\"st_i94_id\",  F.row_number().over(w))\n",
    "#    .withColumn('st_i94_id_new', F.monotonically_increasing_id())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_st_i94_immigrations.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compare the counts of the full dataset again\n",
    "# Count of rows         : 12.228.839\n",
    "# Count of distinct rows: 12.228.839\n",
    "print('Count of rows: {0}'.format(df_st_i94_immigrations.count()))\n",
    "print('Count of distinct rows: {0}'.format(df_st_i94_immigrations.distinct().count()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# *** OPTIONAL 1 ***\n",
    "# Let's check whether there are any duplicates in the data irrespective of `st_i94_id`.\n",
    "# Only columns other than the `st_i94_id` column:\n",
    "\n",
    "print('Count of ids: {0}'.format(df_st_i94_immigrations.count()))\n",
    "print('Count of distinct ids: {0}'.format(\n",
    "    df_st_i94_immigrations.select( [\n",
    "        c for c in df_st_i94_immigrations.columns if c != 'st_i94_id'\n",
    "    ])\n",
    "        .distinct()\n",
    "        .count())\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# *** OPTIONAL 2 ***\n",
    "# clean up if found duplicate rows irrespective of 'st_i94_id'\n",
    "df_st_i94_immigrations = df_st_i94_immigrations.dropDuplicates(subset=[\n",
    "c for c in df_st_i94_immigrations.columns if c != 'st_i94_id'\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Avoid duplicates in ID column `st_i94_id`\n",
    "df_st_i94_immigrations.agg(\n",
    "    F.count('st_i94_id').alias('count'),\n",
    "    F.countDistinct('st_i94_id').alias('distinct')\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check percentage of missing observations are there in each column:\n",
    "df_st_i94_immigrations.agg(*[\n",
    "(1 - (F.count(c) / F.count('*'))).alias(c + '_missing')\n",
    "for c in df_st_i94_immigrations.columns\n",
    "]).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check whether there are still zero values in the result data frame\n",
    "df_st_i94_immigrations\\\n",
    ".select([count( when(col(c).isNull(), c) )\n",
    "        .alias(c) for c in df_st_i94_immigrations.columns])\\\n",
    ".toPandas().T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get current Schema of staging table st_i94_immigration\n",
    "df_st_i94_immigrations.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check content of current staging table st_i94_immigration\n",
    "df_st_i94_immigrations.limit(2).toPandas().T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NOTE:** The column `st_i94_port_state_code` will be inserted after creation of the staging\n",
    "table `st_immigration_airports` within the step [**2 - Airport dimension**](#question2_data_pipeline)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write data and append all month to the same parquet result set\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_stage/PQ1/st_i94_immigrations\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "# write data frame as parquet file (40 Mio. Rows: ~601MB (GZIP) or 855 MB (uncompressed); 12 Mio. Rows: 101 MB (GZIP))\n",
    "# NOTE: One column is still missing: `st_i94_port_state_code`.\n",
    "df_st_i94_immigrations \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .partitionBy('st_i94_year', 'st_i94_month') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")\n",
    "\n",
    "\"\"\"\n",
    "df_st_i94_immigrations\\\n",
    "    .write\\\n",
    "    .format('parquet') \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy('st_i94_year', 'st_i94_month', 'st_i94_port') \\\n",
    "    .saveAsTable('st_i94_immigrations',\n",
    "                 format='parquet',\n",
    "                 mode='overwrite',\n",
    "                 compression=\"gzip\",\n",
    "                 path=filepath_st_i94_immigrations\n",
    "                 )\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Clean data and create staging table `st_immigration_countries` from file [`I94_SAS_Labels_I94CIT_I94RES.txt`](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94CIT_I94RES.txt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# path of txt file\n",
    "filepath_immigration_countries = \"../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94CIT_I94RES.txt\"\n",
    "\n",
    "# read txt file into data frame\n",
    "df_txt_immigration_countries=spark.read.text(filepath_immigration_countries)\n",
    "\n",
    "# create a new df with two columns (st_id_country_code, st_ic_country_name) as staging table st_immigration_countries\n",
    "df_st_immigration_countries = df_txt_immigration_countries\\\n",
    "    .select(F.regexp_extract('value', r'^\\s*(\\d*)\\s*=  \\'(\\w*.*)\\'', 1).alias('st_ic_country_code').cast(IntegerType()),\n",
    "            F.regexp_extract('value', r'^\\s*(\\d*)\\s*=  \\'(\\w*.*)\\'', 2).alias('st_ic_country_name'))\\\n",
    "    .drop_duplicates()\\\n",
    "    .sort(\"st_ic_country_code\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show prepared staging table st_immigration_countries\n",
    "df_st_immigration_countries.sort(\"st_ic_country_code\").show(500, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compare the counts of the full dataset\n",
    "# Count of rows         : 289\n",
    "# Count of distinct rows: 289\n",
    "print('Count of rows: {0}'.format(df_st_immigration_countries.count()))\n",
    "print('Count of distinct rows: {0}'.format(df_st_immigration_countries.distinct().count()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# clean up complete identical rows. Only do this if the results from the step above are not identical!\n",
    "df_st_immigration_countries = df_st_immigration_countries.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show prepared staging table st_immigration_countries\n",
    "df_st_immigration_countries.sort(\"st_ic_country_code\").show(500, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write staging table st_immigration_countries as parquet file\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_stage/PQ1/st_immigration_countries\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_st_immigration_countries \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Creation of a fact table named `f_i94_immigrations` based on staging table `st_i94_immigrations`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read written data frame back into memory\n",
    "df_st_i94_immigrations = spark.read.parquet(\"../P8_capstone_resource_files/parquet_stage/PQ1/st_i94_immigrations\")\n",
    "\n",
    "# show current Schema\n",
    "df_st_i94_immigrations.printSchema()\n",
    "df_st_i94_immigrations.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create fact table f_i94_immigration based of staging table st_i94_immigration\n",
    "df_f_i94_immigrations = df_st_i94_immigrations\\\n",
    "    .withColumnRenamed(\"st_i94_id\", \"f_i94_id\")\\\n",
    "    .withColumnRenamed(\"st_i94_cit\", \"f_i94_cit\")\\\n",
    "    .withColumnRenamed(\"st_i94_addr\", \"f_i94_addr\")\\\n",
    "    .withColumnRenamed(\"st_i94_arrdate\", \"f_i94_arrdate\")\\\n",
    "    .withColumnRenamed(\"st_i94_arrdate_iso\", \"f_i94_arrdate_iso\")\\\n",
    "    .withColumnRenamed(\"st_i94_depdate\", \"f_i94_depdate\")\\\n",
    "    .withColumnRenamed(\"st_i94_depdate_iso\", \"f_i94_depdate_iso\")\\\n",
    "    .withColumnRenamed(\"st_i94_dtadfile\", \"f_i94_dtadfile\")\\\n",
    "    .withColumnRenamed(\"st_i94_matflag\", \"f_i94_matflag\")\\\n",
    "    .withColumnRenamed(\"st_i94_count\", \"f_i94_count\")\\\n",
    "    .withColumnRenamed(\"st_i94_year\", \"f_i94_year\")\\\n",
    "    .withColumnRenamed(\"st_i94_month\", \"f_i94_month\")\\\n",
    "    .withColumnRenamed(\"st_i94_port\", \"f_i94_port\")\\\n",
    "    .withColumn(\"d_ic_id\", col(\"f_i94_cit\"))\\\n",
    "    .withColumn(\"d_ia_id\", col(\"f_i94_port\")) \\\n",
    "    .withColumn(\"d_da_id\", col(\"f_i94_arrdate_iso\")) \\\n",
    "    .withColumn(\"d_dd_id\", col(\"f_i94_depdate_iso\")) \\\n",
    "    .drop(\"f_i94_arrdate\")\\\n",
    "    .drop(\"f_i94_depdate\")\n",
    "\n",
    "# show current fact table Schema\n",
    "df_f_i94_immigrations.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take a look inside the fact table f_i94_immigration\n",
    "df_f_i94_immigrations.show(5,False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write fact table f_i94_immigration based on staging table st_i94_immigration (~ 69 MB)\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_star/PQ1/f_i94_immigrations\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_f_i94_immigrations \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .partitionBy(\"f_i94_year\", \"f_i94_month\")\\\n",
    "    .parquet(location_to_write, compression=\"gzip\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Creation of a dimension named `d_immigration_countries` based on staging table `st_immigration_countries`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read written data frame back into memory\n",
    "df_st_i94_immigration_countries = spark.read.parquet(\"../P8_capstone_resource_files/parquet_stage/PQ1/st_immigration_countries\")\n",
    "\n",
    "# show current Schema\n",
    "df_st_i94_immigration_countries.printSchema()\n",
    "\n",
    "\n",
    "# create dimension table d_i94_immigration_countries based of staging table st_i94_immigration_countries\n",
    "df_d_i94_immigration_countries = df_st_i94_immigration_countries\\\n",
    "    .withColumn(\"d_ic_id\", col(\"st_ic_country_code\"))\\\n",
    "    .withColumnRenamed(\"st_ic_country_code\", \"d_ic_country_code\")\\\n",
    "    .withColumnRenamed(\"st_ic_country_name\", \"d_ic_country_name\")\n",
    "\n",
    "# get current content of dimension table\n",
    "df_d_i94_immigration_countries.printSchema()\n",
    "df_d_i94_immigration_countries.sort(\"d_ic_id\").show(5, False)\n",
    "\n",
    "\n",
    "# write fact table f_i94_immigration based on staging table st_i94_immigration\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_star/PQ1/d_immigration_countries\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_d_i94_immigration_countries \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Mapping of dimension `d_immigration_countries` to  fact table `f_i94_immigration` based on columns\n",
    "   (`st_i94_immigration.st_i94_cit` --> `f_i94_immigration.d_ic_id`) == (`st_immigration_countries.st_ic_country_code`\n",
    "   --> `d_immigration_countries.d_ic_id` )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Answer Project Question 1: From which country do immigrants come to the U.S. and how many?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read written data frame back into memory\n",
    "df_f_i94_immigrations = spark.read.parquet(\"../P8_capstone_resource_files/parquet_star/PQ1/f_i94_immigrations\")\n",
    "df_d_immigration_countries = spark.read.parquet(\"../P8_capstone_resource_files/parquet_star/PQ1/d_immigration_countries\")\n",
    "\n",
    "# check read data frames\n",
    "df_f_i94_immigrations.printSchema()\n",
    "df_d_immigration_countries.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Register data frames as Views\n",
    "df_f_i94_immigrations.createOrReplaceTempView(\"f_i94_immigrations\")\n",
    "df_d_immigration_countries.createOrReplaceTempView(\"d_immigration_countries\")\n",
    "\n",
    "\n",
    "# SQL to answer project question 1 (From which country do immigrants come to the U.S. and how many?)\n",
    "df_pq1 = spark.sql(\"select f_i94.f_i94_cit as county_id\"\n",
    "                   \"     , d_ic.d_ic_country_name as country\"\n",
    "                   \"     , count(f_i94.f_i94_count) as immigrants\"\n",
    "                   \"     , RANK() OVER (ORDER BY count(f_i94.f_i94_count) desc) Immigrants_rank\"\n",
    "                   \"  from f_i94_immigrations f_i94\"\n",
    "                   \"  join d_immigration_countries d_ic on d_ic.d_ic_id = f_i94.d_ic_id\"\n",
    "                   \" group by f_i94.f_i94_cit\"\n",
    "                   \"         ,d_ic.d_ic_country_name\"\n",
    "                   \"  order by Immigrants_rank\"\n",
    "                   \"\")\n",
    "\n",
    "# Show top 10 countries where Immigrants come from and how many\n",
    "df_pq1.filter(df_pq1.Immigrants_rank < 11).show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 4.1.2. At what airports do foreign persons arrive for immigration to the U.S.? [(Description)](#question2_description) <a name=\"question2_data_pipeline\">\n",
    "**Airport dimension**\n",
    "1. Clean data and create staging table `st_immigration_airports` from file\n",
    "   [`I94_SAS_Labels_I94PORT.txt`](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94PORT.txt)\n",
    "   with the columns `st_ia_airport_code` as referencing column, `st_ia_airport_name` and `st_ia_airport_state_code`.\n",
    "\n",
    "    Note that the I-94 airport code is **not** the same as the [IATA](https://en.wikipedia.org/wiki/International_Air_Transport_Association) code and\n",
    "    does not correspond to it. Therefore, `SFR` (I94: 'SFR' = 'SAN FRANCISCO, CA') is used for San\n",
    "    Francisco Airport in this scenario instead of `SFO`. `SFR` means normally San Fernando, CA, USA.\n",
    "\n",
    "    **Project decision:** Data from file [airport-codes.csv](../P8_capstone_resource_files/airport-codes_csv.csv) will **not** be linked to the\n",
    "    I-94 airport codes because incorrect assignments should not be made."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next Steps: Carefully clean list of airports\n",
    "1. read all available information from file\n",
    "2. filter all elements on different regex conditions and store them into a new data frame called `df_st_immigration_airports`\n",
    "3. store cleaned data frame `df_st_immigration_airports` to disk\n",
    "\"\"\"\n",
    "\n",
    "# path of txt file\n",
    "filepath_immigration_airports = \"../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94PORT.txt\"\n",
    "\n",
    "# read txt file into data frame\n",
    "df_txt_immigration_airports_raw = spark.read.text(filepath_immigration_airports)\n",
    "\n",
    "# get regex_cleaned values --> less error prone --> 582 Entries\n",
    "regex_cleaned = r\"^\\s+'([.\\w{2,3} ]*)'\\s+=\\s+'([\\w -.\\/]*),\\s* ([\\w\\/]+)\"\n",
    "\n",
    "df_st_immigration_airports_regex_cleaned = df_txt_immigration_airports_raw\\\n",
    "    .select( F.regexp_extract('value',regex_cleaned, 1).alias('st_ia_airport_code'),\n",
    "             F.regexp_extract('value',regex_cleaned, 2).alias('st_ia_airport_name'),\n",
    "             F.regexp_extract('value',regex_cleaned, 3).alias('st_ia_airport_state_code')) \\\n",
    "    .drop_duplicates() \\\n",
    "    .filter(\"st_ia_airport_code != ''\")  \\\n",
    "    .sort(\"st_ia_airport_state_code\", \"st_ia_airport_code\") \\\n",
    "    .select(\"st_ia_airport_code\", \"st_ia_airport_name\", \"st_ia_airport_state_code\")\n",
    "\n",
    "print(df_st_immigration_airports_regex_cleaned.count())\n",
    "df_st_immigration_airports_regex_cleaned.show(10, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get regex_all values --> with errors like `Collapsed (BUF)` --> 660 Entries\n",
    "regex = r\"^\\s+'([.\\w{2,3} ]*)'\\s+=\\s+'([\\w -.\\/]*)\\s*,*\\s* ([\\w\\/]+)\"\n",
    "\n",
    "df_st_immigration_airports = df_txt_immigration_airports_raw\\\n",
    "    .select( F.regexp_extract('value',regex, 1).alias('st_ia_airport_code'),\n",
    "             F.regexp_extract('value',regex, 2).alias('st_ia_airport_name'),\n",
    "             F.regexp_extract('value',regex, 3).alias('st_ia_airport_state_code')) \\\n",
    "    .drop_duplicates() \\\n",
    "    .filter(\"st_ia_airport_code != ''\")  \\\n",
    "    .sort(\"st_ia_airport_state_code\", \"st_ia_airport_code\")\n",
    "\n",
    "print(df_st_immigration_airports.count())\n",
    "df_st_immigration_airports.show(1000, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Difference of the remaining entries ==> 660 - 582 = 78\n",
    "df_st_immigration_airports \\\n",
    "    .join(df_st_immigration_airports_regex_cleaned,\n",
    "          df_st_immigration_airports.st_ia_airport_code == df_st_immigration_airports_regex_cleaned.st_ia_airport_code,\n",
    "          'left_anti')  \\\n",
    "    .show(10000, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# correct all entries that are not error-free as expected\n",
    "df_st_immigration_airports = df_st_immigration_airports \\\n",
    "    .select(\"st_ia_airport_code\",\n",
    "            F.regexp_replace('st_ia_airport_name', r'Collapsed \\(\\w+\\)|No PORT|UNKNOWN', 'Invalid Airport Entry').alias(\"st_ia_airport_name\"),\n",
    "            F.regexp_replace(\"st_ia_airport_state_code\", r'06/15|Code|POE', 'Invalid State Code').alias(\"st_ia_airport_state_code\")) \\\n",
    "    .select(\"st_ia_airport_code\",\n",
    "            F.regexp_replace('st_ia_airport_name', r\"^DERBY LINE,.*\", \"DERBY LINE, VT (RT. 5)\").alias(\"st_ia_airport_name\"),\n",
    "            F.regexp_replace(\"st_ia_airport_state_code\", r\"5\", \"VT\").alias(\"st_ia_airport_state_code\")) \\\n",
    "    .select(\"st_ia_airport_code\",\n",
    "            F.regexp_replace('st_ia_airport_name', r\"^LOUIS BOTHA, SOUTH\", \"LOUIS BOTHA\").alias(\"st_ia_airport_name\"),\n",
    "            F.regexp_replace(\"st_ia_airport_state_code\", r\"AFRICA\", \"SOUTH AFRICA\").alias(\"st_ia_airport_state_code\")) \\\n",
    "    .select(\"st_ia_airport_code\",\n",
    "            F.regexp_replace('st_ia_airport_name', r\",\", \"\").alias(\"st_ia_airport_name\"),\n",
    "            \"st_ia_airport_state_code\") \\\n",
    "    .select(\"st_ia_airport_code\",\n",
    "            F.regexp_replace('st_ia_airport_name', r\"^PASO DEL\", \"PASO DEL NORTE\").alias(\"st_ia_airport_name\"),\n",
    "            F.regexp_replace(\"st_ia_airport_state_code\", r\"NORTE\", \"TX\").alias(\"st_ia_airport_state_code\")) \\\n",
    "    .select(\"st_ia_airport_code\",\n",
    "            F.regexp_replace('st_ia_airport_name', r\"^UNIDENTIFED AIR /?\", \"Invalid Airport Entry\").alias(\"st_ia_airport_name\"),\n",
    "            F.regexp_replace(\"st_ia_airport_state_code\", r\"^SEAPORT?\", \"Invalid State Code\").alias(\"st_ia_airport_state_code\")) \\\n",
    "    .select(\"st_ia_airport_code\",\n",
    "            F.regexp_replace('st_ia_airport_name', r\"Abu\", \"Abu Dhabi\").alias(\"st_ia_airport_name\"),\n",
    "            F.regexp_replace(\"st_ia_airport_state_code\", r\"Dhabi\", \"Invalid State Code\").alias(\"st_ia_airport_state_code\")) \\\n",
    "    .select(\"st_ia_airport_code\",\n",
    "            F.regexp_replace('st_ia_airport_name', r\"DOVER-AFB\", \"Invalid Airport Entry\").alias(\"st_ia_airport_name\"),\n",
    "            F.regexp_replace(\"st_ia_airport_state_code\", r\"DE\", \"Invalid State Code\").alias(\"st_ia_airport_state_code\")) \\\n",
    "    .select(\"st_ia_airport_code\",\n",
    "            F.regexp_replace('st_ia_airport_name', r\"NOT REPORTED/UNKNOWNGALES\", \"NOGALES\").alias(\"st_ia_airport_name\"),\n",
    "            F.regexp_replace(\"st_ia_airport_state_code\", r\"AZ\", \"AZ\").alias(\"st_ia_airport_state_code\")) \\\n",
    "    .select(\"st_ia_airport_code\",\n",
    "            F.regexp_replace('st_ia_airport_name', r\"^NOT\", \"Invalid Airport Entry\").alias(\"st_ia_airport_name\"),\n",
    "            F.regexp_replace(\"st_ia_airport_state_code\", r\"REPORTED/UNKNOWN\", \"Invalid State Code\").alias(\"st_ia_airport_state_code\")) \\\n",
    "    .select(\"st_ia_airport_code\",\n",
    "            F.regexp_replace('st_ia_airport_name', r\"INVALID - IWAKUNI\", \"IWAKUNI\").alias(\"st_ia_airport_name\"),\n",
    "            F.regexp_replace(\"st_ia_airport_state_code\", r\"JAPAN\", \"JAPAN\").alias(\"st_ia_airport_state_code\")) \\\n",
    "    .sort(\"st_ia_airport_name\", \"st_ia_airport_code\")\n",
    "\n",
    "print(df_st_immigration_airports.count())\n",
    "df_st_immigration_airports.show(1000, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check if former invalid entries are cleaned correctly\n",
    "# Difference of the remaining entries ==> 660 - 582 = 78\n",
    "df_st_immigration_airports \\\n",
    "    .join(df_st_immigration_airports_regex_cleaned,\n",
    "          df_st_immigration_airports.st_ia_airport_code == df_st_immigration_airports_regex_cleaned.st_ia_airport_code, 'left_anti')  \\\n",
    "    .show(10000, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Write data as new CSV file to disk\n",
    "location_to_write = '../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/st_immigration_airports.csv'\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_st_immigration_airports \\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(location_to_write, header = 'true')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write df_st_immigration_airports back to stage area on file system\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_stage/PQ2/st_immigration_airports\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_st_immigration_airports \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read written data frame back into memory\n",
    "# st_immigration_airports:\n",
    "location_st_immigration_airports = \"../P8_capstone_resource_files/parquet_stage/PQ2/st_immigration_airports\"\n",
    "df_st_immigration_airports = spark.read.parquet(location_st_immigration_airports)\n",
    "\n",
    "# current Schema of staging table st_immigration_airports\n",
    "print(df_st_immigration_airports.count())\n",
    "df_st_immigration_airports.printSchema()\n",
    "df_st_immigration_airports.show(10, False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Add the column `st_ia_airport_state_code --> st_i94_port_state_code` to staging table `st_i94_immigration` based on staging\n",
    "   table `st_immigration_airports`. This information is needed to connect the `us-cities-demographics.json` file later on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read df_st_i94_immigrations staging table and add column `st_i94_port_state_code` to it. Write data frame back to disk.\n",
    "\n",
    "# Read written data frame back into memory\n",
    "# st_i94_immigrations:\n",
    "location_st_i94_immigrations = \"../P8_capstone_resource_files/parquet_stage/PQ1/st_i94_immigrations\"\n",
    "df_st_i94_immigrations = spark.read.parquet(location_st_i94_immigrations)\n",
    "\n",
    "# st_immigration_airports:\n",
    "location_st_immigration_airports = \"../P8_capstone_resource_files/parquet_stage/PQ2/st_immigration_airports\"\n",
    "df_st_immigration_airports = spark.read.parquet(location_st_immigration_airports)\n",
    "\n",
    "\n",
    "print(df_st_i94_immigrations.count())\n",
    "df_st_i94_immigrations.printSchema()\n",
    "df_st_i94_immigrations.show(5, False)\n",
    "\n",
    "print(df_st_immigration_airports.count())\n",
    "df_st_immigration_airports.printSchema()\n",
    "df_st_immigration_airports.show(5, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# check if st_i94_dept_date_iso is 1900-01-01 (default value - No onward travel is planned)\n",
    "df_st_i94_immigrations \\\n",
    "    .filter(df_st_i94_immigrations.st_i94_depdate == 0)\\\n",
    "    .show(5, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add column `st_i94_port_state_code` to data frame st_i94_immigrations\n",
    "df_st_i94_immigrations = df_st_i94_immigrations \\\n",
    "    .join(df_st_immigration_airports,\n",
    "          [df_st_i94_immigrations.st_i94_port == df_st_immigration_airports.st_ia_airport_code], 'left_outer') \\\n",
    "    .drop(\"st_ia_airport_code\", \"st_ia_airport_name\") \\\n",
    "    .withColumnRenamed(\"st_ia_airport_state_code\", \"st_i94_port_state_code\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check if `st_i94_port_state_code` has null values\n",
    "df_st_i94_immigrations\\\n",
    "    .fillna(value='NA', subset=['st_i94_port_state_code'])\\\n",
    "    .groupBy(\"st_i94_port_state_code\")\\\n",
    "    .count() \\\n",
    "    .sort(\"st_i94_port_state_code\")\\\n",
    "    .orderBy(\"count\")\\\n",
    "    .show(500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get entry with null value\n",
    "df_st_i94_immigrations \\\n",
    "    .filter(col(\"st_i94_port_state_code\").isNull()).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rename"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get status\n",
    "print(df_st_i94_immigrations.count())\n",
    "df_st_i94_immigrations.printSchema()\n",
    "df_st_i94_immigrations.show(5, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write st_i94_immigrations back to file system\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_stage/PQ2/st_i94_immigrations\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_st_i94_immigrations \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .partitionBy('st_i94_year', 'st_i94_month') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Add new column `st_i94_port_state_code --> f_i94_port_state_code` to existing fact table `f_i94_immigrations`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read data frames back into memory\n",
    "# st_i94_immigrations with column `st_i94_port_state_code`:\n",
    "location_st_i94_immigrations = \"../P8_capstone_resource_files/parquet_stage/PQ2/st_i94_immigrations\"\n",
    "df_st_i94_immigrations = spark.read.parquet(location_st_i94_immigrations)\n",
    "\n",
    "# f_i94_immigrations:\n",
    "location_f_i94_immigrations = \"../P8_capstone_resource_files/parquet_star/PQ1/f_i94_immigrations\"\n",
    "df_f_i94_immigrations = spark.read.parquet(location_f_i94_immigrations)\n",
    "\n",
    "# show current schemas\n",
    "print(df_st_i94_immigrations.count())\n",
    "df_st_i94_immigrations.printSchema()\n",
    "\n",
    "print(df_f_i94_immigrations.count())\n",
    "df_f_i94_immigrations.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get only the needed columns to join\n",
    "df_st_i94_immigrations_2_join = df_st_i94_immigrations \\\n",
    "    .select(\"st_i94_id\", \"st_i94_port_state_code\")\n",
    "\n",
    "\n",
    "# add new columns to fact table `df_f_i94_immigrations`\n",
    "df_f_i94_immigrations = df_f_i94_immigrations  \\\n",
    "    .join(df_st_i94_immigrations_2_join, df_f_i94_immigrations.f_i94_id == df_st_i94_immigrations_2_join.st_i94_id, 'inner') \\\n",
    "    .drop(\"st_i94_id\") \\\n",
    "    .withColumnRenamed(\"st_i94_port_state_code\", \"f_i94_port_state_code\") \\\n",
    "    .withColumn(\"d_sd_id\", col(\"f_i94_addr\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_f_i94_immigrations.printSchema()\n",
    "df_f_i94_immigrations.show(5, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write fact table f_i94_immigration (~ 109,7 MB)\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_star/PQ2/f_i94_immigrations\"\n",
    "\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_f_i94_immigrations \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .partitionBy(\"f_i94_year\", \"f_i94_month\")\\\n",
    "    .parquet(location_to_write, compression=\"gzip\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Creation of a dimension named `d_immigration_airports` based on staging table `st_immigration_airports`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# st_immigration_airports:\n",
    "location_st_immigration_airports = \"../P8_capstone_resource_files/parquet_stage/PQ2/st_immigration_airports\"\n",
    "df_d_immigration_airports = spark.read.parquet(location_st_immigration_airports)\n",
    "\n",
    "print(df_d_immigration_airports.count())\n",
    "df_d_immigration_airports.printSchema()\n",
    "df_d_immigration_airports.show(5, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_d_immigration_airports = df_d_immigration_airports  \\\n",
    "    .withColumn(\"d_ia_id\", df_d_immigration_airports.st_ia_airport_code) \\\n",
    "    .withColumnRenamed(\"st_ia_airport_code\", \"d_ia_airport_code\") \\\n",
    "    .withColumnRenamed(\"st_ia_airport_name\", \"d_ia_airport_name\") \\\n",
    "    .withColumnRenamed(\"st_ia_airport_state_code\", \"d_ia_airport_state_code\")\n",
    "\n",
    "df_d_immigration_airports.printSchema()\n",
    "df_d_immigration_airports.show(5, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write dimension table d_immigration_airports to disk (~ 10 kB)\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_star/PQ2/d_immigration_airports\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_d_immigration_airports \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Mapping of dimension `d_immigration_airports` to  fact table `f_i94_immigration` based on columns\n",
    "   (`st_immigration_airports.st_ia_airport_code` --> `d_immigration_airports.d_ia_id`) ==\n",
    "   (`st_i94_immigration.st_i94_port` --> `f_i94_immigration.d_ia_id`)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Answer Project Question 2: At what airports do foreign persons arrive for immigration to the U.S.?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read written data frame back into memory\n",
    "df_f_i94_immigrations = spark.read.parquet(\"../P8_capstone_resource_files/parquet_star/PQ2/f_i94_immigrations\")\n",
    "df_d_immigration_airports = spark.read.parquet(\"../P8_capstone_resource_files/parquet_star/PQ2/d_immigration_airports\")\n",
    "\n",
    "# check read data frames\n",
    "print(df_f_i94_immigrations.count())\n",
    "df_f_i94_immigrations.printSchema()\n",
    "df_f_i94_immigrations.show(5, False)\n",
    "\n",
    "print(df_d_immigration_airports.count())\n",
    "df_d_immigration_airports.printSchema()\n",
    "df_d_immigration_airports.show(5, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Register data frames as Views\n",
    "df_f_i94_immigrations.createOrReplaceTempView(\"f_i94_immigrations\")\n",
    "df_d_immigration_airports.createOrReplaceTempView(\"d_immigration_airports\")\n",
    "\n",
    "# SQL to answer project question 2 (From which country do immigrants come to the U.S. and how many?)\n",
    "df_pq2 = spark.sql(\" select   d_ia.d_ia_airport_code as airport_code\"\n",
    "                     \"       ,d_ia.d_ia_airport_name as airport_name\"\n",
    "                     \"       ,d_ia.d_ia_airport_state_code as airport_state_code\"\n",
    "                     \"       ,sum(f_i94.f_i94_count) as immigrants\"\n",
    "                     \"       ,RANK() OVER (ORDER BY count(f_i94.f_i94_count) desc) Immigration_airport_rank\"\n",
    "                     \" from f_i94_immigrations f_i94\"\n",
    "                     \" join d_immigration_airports d_ia on f_i94.d_ia_id = d_ia.d_ia_id\"\n",
    "                     \" group by airport_code\"\n",
    "                     \"       , airport_name\"\n",
    "                     \"       , airport_state_code\"\n",
    "                     \" order by Immigration_airport_rank asc \")\n",
    "\n",
    "df_pq2.show(5000, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 4.1.3. At what times do foreign persons arrive for immigration to the U.S.? [(Data pipeline)](#question3_description) <a name=\"question3_data_pipeline\">\n",
    "**Date dimensions**\n",
    "\n",
    "`st_i94_arrdate` and `st_i94_depdate` from staging table `st_i94_immigration` describe dates in SAS specific Date format.\n",
    "The SAS date calculation starts on 1960-01-01. These columns are converted to DateType format in the staging table\n",
    "`st_i94_immigrations` as columns named `st_i94_arrdate_iso` and `st_i94_arrdate_iso`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get date values from columns `st_i94_immigration.st_i94_arrdate_iso` and `st_i94_immigration.st_i94_depdate_iso`.\n",
    "Get a valid MIN(), MAX() and default (null value representation) date. Clean data and rewrite staging table 'st_i94_immigrations' if needed.\n",
    "Finally, create two dimensions 'd_date_arrivals' and 'd_date_departures' out of it without gaps.\n",
    "\n",
    "1. Read data and get min() and max() value out of `st_i94_arrdate_iso` and `st_i94_depdate_iso`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read written data frame back into memory\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_stage/PQ2/st_i94_immigrations\"\n",
    "df_st_i94_immigrations = spark.read.parquet(location_to_read)\n",
    "df_st_i94_immigrations.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get an overview about valid data - check some different perspectives\n",
    "# get valid min and max date from date_fields `st_i94_arrdate_iso` and `st_i94_depdate_iso`\n",
    "\n",
    "st_i94_arrdate_depdate_iso = df_st_i94_immigrations.select(\n",
    "    F.min(col(\"st_i94_arrdate_iso\")).alias(\"st_i94_arrdate_iso_min\"),\n",
    "    F.max(col(\"st_i94_arrdate_iso\")).alias(\"st_i94_arrdate_iso_max\"),\n",
    "    F.min(col(\"st_i94_depdate_iso\")).alias(\"st_i94_depdate_iso_min\"),\n",
    "    F.max(col(\"st_i94_depdate_iso\")).alias(\"st_i94_depdate_iso_max\"),\n",
    ")\n",
    "\n",
    "print(st_i94_arrdate_depdate_iso)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get an overview about arrdate:\n",
    "# all distinct date values\n",
    "print(df_st_i94_immigrations.select(\"st_i94_arrdate_iso\").distinct().count())\n",
    "# Most entries on which date?\n",
    "df_st_i94_immigrations.select(\"st_i94_arrdate_iso\").groupBy(\"st_i94_arrdate_iso\").count().sort(\"count\", ascending=False).show(1000, False)\n",
    "# get all date values. Is there a large gap or date values out of range?\n",
    "df_st_i94_immigrations.select(\"st_i94_arrdate_iso\").groupBy(\"st_i94_arrdate_iso\").count().sort(\"st_i94_arrdate_iso\", ascending=True).show(1000, False)\n",
    "\n",
    "\"\"\"\n",
    "Findings:\n",
    "\n",
    "Everything seems to be valid. Date values start from 1st of January 2016 and ends by 31st of December 2016.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get an overview about depdate:\n",
    "# all distinct date values\n",
    "print(df_st_i94_immigrations.select(\"st_i94_depdate_iso\").distinct().count())\n",
    "# Most entries on which date?\n",
    "df_st_i94_immigrations.select(\"st_i94_depdate_iso\").groupBy(\"st_i94_depdate_iso\").count().sort(\"count\", ascending=False).show(10, False)\n",
    "# get all date values. Is there a large gap or date values out of range?\n",
    "df_st_i94_immigrations.select(\"st_i94_depdate_iso\").groupBy(\"st_i94_depdate_iso\").count().sort(\"st_i94_depdate_iso\", ascending=True).show(1000, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compare st_i94_arrdate with st_i94_depdate. Is departure date earlier than arrival date --> there is a logical failure!\n",
    "\n",
    "# Show only data to be corrected in the third column\n",
    "df_st_i94_immigrations \\\n",
    "    .groupBy(\"st_i94_arrdate_iso\", \"st_i94_arrdate\", \"st_i94_depdate\", \"st_i94_depdate_iso\") \\\n",
    "    .count() \\\n",
    "    .withColumn(\"st_i94_depdate_iso_wrong_dates\",\n",
    "                 when(col(\"st_i94_depdate_iso\") < \"2016-01-01\", \"1111-01-01\")\\\n",
    "                .when(col(\"st_i94_depdate_iso\") > \"2017-06-14\", \"2222-01-01\") \\\n",
    "                .when(col(\"st_i94_arrdate_iso\") > col(\"st_i94_depdate_iso\"), \"3333-01-01\")\n",
    "                .otherwise(\" \").cast(StringType())) \\\n",
    "    .orderBy(\"st_i94_arrdate_iso\", \"st_i94_depdate_iso\") \\\n",
    "    .show(5000) \\\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Findings:\n",
    "715 different date values ==> all distinct date values are greater than (>) 366 days of a year\n",
    "==> That's possible. Many Immigrants already know their departure date.\n",
    "\n",
    "1900-01-01 (start date):  This date is used as default value instead of a null value\n",
    "\n",
    "arrdate starts on 2016-01-01 ==> The departure date cannot be earlier than the arrival date! --> each date before\n",
    "2016-01-01 must be set to 1900-01-01 as null/default value\n",
    "\n",
    "depdate greater than 2017-06-14 is not realistic, due to the very small amount of depdate entries within this range of dates\n",
    "==> entries must be set to 1900-01-01 (null/default)\n",
    "\n",
    "arrdate describes the 1st arrival into the U.S.. After that the immigrants decide to travel to different states in the U.S..\n",
    "conclusion: arrdate must be earlier than depdate (arrdate < depdate ==> 2016-01-01 < 2016-01-02)\n",
    "\n",
    "The following table shows some wrong dates where arrdate > depdate\n",
    "+------------------+--------------+--------------+------------------+-----+------------------------------+\n",
    "|st_i94_arrdate_iso|st_i94_arrdate|st_i94_depdate|st_i94_depdate_iso|count|st_i94_depdate_iso_wrong_dates|\n",
    "+------------------+--------------+--------------+------------------+-----+------------------------------+\n",
    "|        2016-01-02|         20455|         20454|        2016-01-01|    1|                    3333-01-01|\n",
    "|        2016-01-08|         20461|         20454|        2016-01-01|    1|                    3333-01-01|\n",
    "|        2016-01-08|         20461|         20459|        2016-01-06|    2|                    3333-01-01|\n",
    "|        2016-01-08|         20461|         20460|        2016-01-07|    3|                    3333-01-01|\n",
    "+------------------+--------------+--------------+------------------+-----+------------------------------+\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Clean date column \"st_i94_depdate_iso\" and \"st_\": Valid entries are between 2016-01-01 and 2017-06-14. Pre- and descending values\n",
    "   will be set to null / default value (1900-01-01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show corrected column `st_i94_depdate_iso_corrected`\n",
    "df_st_i94_immigrations \\\n",
    "    .groupBy(\"st_i94_arrdate_iso\", \"st_i94_arrdate\", \"st_i94_depdate\", \"st_i94_depdate_iso\") \\\n",
    "    .count() \\\n",
    "    .withColumn(\"st_i94_depdate_iso_corrected\",\n",
    "                 when(col(\"st_i94_depdate_iso\") < \"2016-01-01\", \"1900-01-01\")\\\n",
    "                .when(col(\"st_i94_depdate_iso\") > \"2017-06-14\", \"1900-01-01\") \\\n",
    "                .when(col(\"st_i94_arrdate_iso\") > col(\"st_i94_depdate_iso\"), \"1900-01-01\")\n",
    "                .otherwise(col(\"st_i94_depdate_iso\")).cast(DateType())) \\\n",
    "    .orderBy(\"st_i94_arrdate_iso\", \"st_i94_depdate_iso\") \\\n",
    "    .show(5000) \\"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# correct the date values in column `st_i94_depdate_iso`\n",
    "df_st_i94_immigrations = df_st_i94_immigrations \\\n",
    "    .withColumn(\"st_i94_depdate_iso\",\n",
    "                 when(col(\"st_i94_depdate_iso\") < \"2016-01-01\", \"1900-01-01\") \\\n",
    "                .when(col(\"st_i94_depdate_iso\") > \"2017-06-14\", \"1900-01-01\") \\\n",
    "                .when(col(\"st_i94_arrdate_iso\") > col(\"st_i94_depdate_iso\"), \"1900-01-01\")\n",
    "                .otherwise(col(\"st_i94_depdate_iso\")).cast(DateType()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_st_i94_immigrations \\\n",
    "    .groupBy(\"st_i94_arrdate_iso\", \"st_i94_arrdate\", \"st_i94_depdate\", \"st_i94_depdate_iso\") \\\n",
    "    .count() \\\n",
    "    .withColumn(\"st_i94_depdate_iso_wrong_dates\",\n",
    "                 when(col(\"st_i94_depdate_iso\") < \"2016-01-01\", \"1111-01-01\")\\\n",
    "                .when(col(\"st_i94_depdate_iso\") > \"2017-06-14\", \"2222-01-01\") \\\n",
    "                .when(col(\"st_i94_arrdate_iso\") > col(\"st_i94_depdate_iso\"), \"3333-01-01\")\n",
    "                .otherwise(\" \").cast(StringType())) \\\n",
    "    .orderBy(\"st_i94_depdate_iso_wrong_dates\", ascending=False) \\\n",
    "    .show(5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% check if wrong dates are available anymore. Only 1111-01-01 is allowed!\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_st_i94_immigrations.printSchema()\n",
    "df_st_i94_immigrations.show(500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write st_i94_immigrations back to file system\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_stage/PQ3/st_i94_immigrations\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_st_i94_immigrations \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .partitionBy('st_i94_year', 'st_i94_month') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Update fact table `f_i94_immigrations` based on cleaned column `st_i94_depdate_iso`  values inside"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read data frames back into memory\n",
    "# st_i94_immigrations with column `st_i94_port_state_code`:\n",
    "location_st_i94_immigrations = \"../P8_capstone_resource_files/parquet_stage/PQ3/st_i94_immigrations\"\n",
    "df_st_i94_immigrations = spark.read.parquet(location_st_i94_immigrations)\n",
    "\n",
    "# f_i94_immigrations:\n",
    "location_f_i94_immigrations = \"../P8_capstone_resource_files/parquet_star/PQ2/f_i94_immigrations\"\n",
    "df_f_i94_immigrations = spark.read.parquet(location_f_i94_immigrations)\n",
    "\n",
    "# show current schemas\n",
    "print(df_st_i94_immigrations.count())\n",
    "df_st_i94_immigrations.printSchema()\n",
    "df_st_i94_immigrations.show(5,False)\n",
    "\n",
    "print(df_f_i94_immigrations.count())\n",
    "df_f_i94_immigrations.printSchema()\n",
    "df_f_i94_immigrations.show(5,False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add column 'st_i94_depdate_iso' to fact table 'f_i94_immigrations'\n",
    "df_st_i94_immigrations_2_join = df_st_i94_immigrations \\\n",
    "    .select(\"st_i94_id\" , \"st_i94_depdate_iso\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_st_i94_immigrations_2_join.printSchema()\n",
    "df_st_i94_immigrations_2_join.show(5,False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_f_i94_immigrations = df_f_i94_immigrations \\\n",
    "    .join(df_st_i94_immigrations_2_join, df_f_i94_immigrations.f_i94_id == df_st_i94_immigrations_2_join.st_i94_id, 'inner') \\\n",
    "    .withColumn(\"f_i94_depdate_iso\", col(\"st_i94_depdate_iso\")) \\\n",
    "    .drop(\"st_i94_id\", \"st_i94_depdate_iso\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_f_i94_immigrations.printSchema()\n",
    "df_f_i94_immigrations.show(5,False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# check if values of referencing column \"d_dd_id\" are equal to column \"f_i94_depdate_iso\"\n",
    "df_f_i94_immigrations \\\n",
    "    .filter(col(\"d_dd_id\") != col(\"f_i94_depdate_iso\")) \\\n",
    "    .groupBy(\"d_dd_id\", \"f_i94_depdate_iso\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"d_dd_id\") \\\n",
    "    .show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# update column 'd_dd_id' with newly updated values from column 'f_i94_depdate_iso'\n",
    "df_f_i94_immigrations = df_f_i94_immigrations \\\n",
    "    .withColumn(\"d_dd_id\", col(\"f_i94_depdate_iso\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check again if values of referencing column \"d_dd_id\" are equal to column \"f_i94_depdate_iso\"\n",
    "df_f_i94_immigrations \\\n",
    "    .filter(col(\"d_dd_id\") != col(\"f_i94_depdate_iso\")) \\\n",
    "    .groupBy(\"d_dd_id\", \"f_i94_depdate_iso\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"d_dd_id\") \\\n",
    "    .show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_f_i94_immigrations.printSchema()\n",
    "df_f_i94_immigrations.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write st_i94_immigrations back to file system\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_star/PQ3/f_i94_immigrations\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_f_i94_immigrations \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite')\\\n",
    "    .partitionBy('f_i94_year', 'f_i94_month') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Generate new date staging tables (`st_date_arrivals`, `st_date_departures`) based on default, min and max values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create new data frame with date series\n",
    "def generate_dates(spark,range_list, dt_col=\"date_time_ref\", interval=60*60*24): # TODO: attention to sparkSession\n",
    "    \"\"\"\n",
    "    ...     Create a Spark DataFrame with a single column named dt_col and a range of date within a specified interval (start and stop included).\n",
    "    ...     With hourly data, dates end at 23 of stop day\n",
    "    ...     (https://stackoverflow.com/questions/57537760/pyspark-how-to-generate-a-dataframe-composed-of-datetime-range)\n",
    "    ...\n",
    "    ...     :param spark: SparkSession or sqlContext depending on environment (server vs local)\n",
    "    ...     :param range_list: array of strings formatted as \"2018-01-20\" or \"2018-01-20 00:00:00\"\n",
    "    ...     :param interval: number of seconds (frequency), output from get_freq()\n",
    "    ...     :param dt_col: string with date column name. Date column must be TimestampType\n",
    "    ...\n",
    "    ...     :returns: df from range\n",
    "    ...     \"\"\"\n",
    "    start,stop = range_list\n",
    "    temp_df = spark.createDataFrame([(start, stop)], (\"start\", \"stop\"))\n",
    "    temp_df = temp_df.select([F.col(c).cast(\"timestamp\") for c in (\"start\", \"stop\")])\n",
    "    temp_df = temp_df.withColumn(\"stop\",F.date_add(\"stop\",1).cast(\"timestamp\"))\n",
    "    temp_df = temp_df.select([F.col(c).cast(\"long\") for c in (\"start\", \"stop\")])\n",
    "    start, stop = temp_df.first()\n",
    "    return spark.range(start,stop,interval).select(F.col(\"id\").cast(\"timestamp\").cast(\"date\").alias(dt_col))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create new staging tables 'st_date_arrivals' and 'st_date_departure' with min and max date values\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ3/f_i94_immigrations\"\n",
    "df_f_i94_immigrations = spark.read.parquet(location_to_read)\n",
    "\n",
    "print(df_f_i94_immigrations.count())\n",
    "df_f_i94_immigrations.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check if all date values from \"f_i94_arrdate_iso\" are valid\n",
    "df_f_i94_immigrations\\\n",
    "    .groupBy(\"f_i94_arrdate_iso\")\\\n",
    "    .count()\\\n",
    "    .orderBy(\"f_i94_arrdate_iso\")\\\n",
    "    .show(5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get min and max values for \"f_i94_arrdate\"\n",
    "f_i94_arrdate_iso_min, f_i94_arrdate_iso_max =  df_f_i94_immigrations \\\n",
    "    .select(F.min(\"f_i94_arrdate_iso\").alias(\"f_i94_arrdate_iso_min\"), \\\n",
    "            F.max(\"f_i94_arrdate_iso\").alias(\"f_i94_arrdate_iso_max\")) \\\n",
    "    .first()\n",
    "\n",
    "\n",
    "print(f\"f_i94_arrdate_iso_min: {f_i94_arrdate_iso_min}\")\n",
    "print(f\"f_i94_arrdate_iso_max: {f_i94_arrdate_iso_max}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create new staging table \"st_date_arrivals\"\n",
    "date_range = [f_i94_arrdate_iso_min, f_i94_arrdate_iso_max]\n",
    "dt_col=\"st_da_date\"\n",
    "df_st_date_arrivals = generate_dates(spark, date_range, dt_col)\n",
    "\n",
    "df_st_date_arrivals.printSchema()\n",
    "df_st_date_arrivals.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_st_date_arrivals.tail(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Append date specific columns to staging tables, create a dimension from it and save it to the file system."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create new columns of st_date_arrivals table\n",
    "# https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "\n",
    "df_st_date_arrivals = df_st_date_arrivals \\\n",
    "    .withColumn(\"st_da_id\", col(\"st_da_date\")) \\\n",
    "    .withColumn(\"st_da_year\", F.year(col(\"st_da_date\"))) \\\n",
    "    .withColumn(\"st_da_year_quarter\", F.concat_ws('/', F.year(col(\"st_da_date\")), F.quarter(col(\"st_da_date\")))) \\\n",
    "    .withColumn(\"st_da_year_month\", F.concat_ws('/', F.year(col(\"st_da_date\")), F.month(col(\"st_da_date\")))) \\\n",
    "    .withColumn(\"st_da_year_month\", F.concat_ws('/', F.year(col(\"st_da_date\")), date_format(col(\"st_da_date\"), 'MM'))) \\\n",
    "    .withColumn(\"st_da_quarter\", F.quarter(col(\"st_da_date\"))) \\\n",
    "    .withColumn(\"st_da_month\", F.month(col(\"st_da_date\"))) \\\n",
    "    .withColumn(\"st_da_week\", F.weekofyear(col(\"st_da_date\"))) \\\n",
    "    .withColumn(\"st_da_weekday\", F.date_format(col(\"st_da_date\"),'EEEE')) \\\n",
    "    .withColumn(\"st_da_weekday_short\", F.date_format(col(\"st_da_date\"),'EEE')) \\\n",
    "    .withColumn(\"st_da_dayofweek\", F.dayofweek(col(\"st_da_date\"))) \\\n",
    "    .withColumn(\"st_da_day\", F.dayofmonth(col(\"st_da_date\")) )\n",
    "\n",
    "df_st_date_arrivals.printSchema()\n",
    "df_st_date_arrivals.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# persist staging time table 'st_date_arrivals'\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_stage/PQ3/st_date_arrivals\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_st_date_arrivals \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create dimension 'd_date_arrivals' from staging table 'st_date_arrivals'\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_stage/PQ3/st_date_arrivals\"\n",
    "df_st_date_arrivals = spark.read.parquet(location_to_read)\n",
    "\n",
    "print(df_st_date_arrivals.count())\n",
    "df_st_date_arrivals.printSchema()\n",
    "\n",
    "df_st_date_arrivals = df_st_date_arrivals \\\n",
    "    .withColumnRenamed(\"st_da_date\", \"d_da_date\") \\\n",
    "    .withColumnRenamed(\"st_da_id\", \"d_da_id\") \\\n",
    "    .withColumnRenamed(\"st_da_year\", \"d_da_year\") \\\n",
    "    .withColumnRenamed(\"st_da_year_quarter\", \"d_da_year_quarter\") \\\n",
    "    .withColumnRenamed(\"st_da_year_month\", \"d_da_year_month\") \\\n",
    "    .withColumnRenamed(\"st_da_quarter\", \"d_da_quarter\") \\\n",
    "    .withColumnRenamed(\"st_da_month\", \"d_da_month\") \\\n",
    "    .withColumnRenamed(\"st_da_week\", \"d_da_week\") \\\n",
    "    .withColumnRenamed(\"st_da_weekday\", \"d_da_weekday\") \\\n",
    "    .withColumnRenamed(\"st_da_weekday_short\", \"d_da_weekday_short\") \\\n",
    "    .withColumnRenamed(\"st_da_dayofweek\", \"d_da_dayofweek\") \\\n",
    "    .withColumnRenamed(\"st_da_day\", \"d_da_day\") \\\n",
    "\n",
    "df_st_date_arrivals.printSchema()\n",
    "df_st_date_arrivals.show(5)\n",
    "\n",
    "\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_star/PQ3/d_date_arrivals\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_st_date_arrivals \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creation of the second dimension named `d_date_departures` based on fact column `f_i94_depdate_iso`.\n",
    "# Create new staging table 'st_date_departure' with min, max and default date values\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ3/f_i94_immigrations\"\n",
    "df_f_i94_immigrations = spark.read.parquet(location_to_read)\n",
    "\n",
    "print(df_f_i94_immigrations.count())\n",
    "df_f_i94_immigrations.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check if all date values from \"f_i94_depdate_iso\" are valid\n",
    "df_f_i94_immigrations\\\n",
    "    .groupBy(\"f_i94_depdate_iso\")\\\n",
    "    .count()\\\n",
    "    .orderBy(\"f_i94_depdate_iso\")\\\n",
    "    .show(5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# extract default, min and max date from column 'f_i94_depdate_iso'\n",
    "# get default and min value\n",
    "f_i94_depdate_iso_default, f_i94_depdate_iso_min = df_f_i94_immigrations\\\n",
    "    .select(\"f_i94_depdate_iso\") \\\n",
    "    .distinct() \\\n",
    "    .orderBy(\"f_i94_depdate_iso\", ascending=True) \\\n",
    "    .limit(2) \\\n",
    "    .select(F.min(\"f_i94_depdate_iso\").alias(\"f_i94_depdate_iso_default\"),\n",
    "            F.max(\"f_i94_depdate_iso\").alias(\"f_i94_depdate_iso_min\")) \\\n",
    "    .first()\n",
    "\n",
    "# get max value\n",
    "f_i94_depdate_iso_max, f_i94_depdate_iso_max =  df_f_i94_immigrations \\\n",
    "    .select(F.max(\"f_i94_depdate_iso\").alias(\"f_i94_depdate_iso_max\"), \\\n",
    "            F.max(\"f_i94_depdate_iso\").alias(\"f_i94_depdate_iso_max\")) \\\n",
    "    .first()\n",
    "\n",
    "# check selected data\n",
    "print(f\"f_i94_depdate_iso_default: {f_i94_depdate_iso_default}\")\n",
    "print(f\"f_i94_depdate_iso_min: {f_i94_depdate_iso_min}\")\n",
    "print(f\"f_i94_depdate_iso_max: {f_i94_depdate_iso_max}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create new staging table \"st_date_departures\"\n",
    "date_range_default = [f_i94_depdate_iso_default, f_i94_depdate_iso_default]\n",
    "date_range_min_max = [f_i94_depdate_iso_min, f_i94_depdate_iso_max]\n",
    "\n",
    "# check valid date ranges\n",
    "print(date_range_default)\n",
    "print(date_range_min_max)\n",
    "\n",
    "# create new data frames for\n",
    "dt_col=\"st_dd_date\"\n",
    "df_st_date_departures_default = generate_dates(spark, date_range_default, dt_col)\n",
    "df_st_date_departures_min_max = generate_dates(spark, date_range_min_max, dt_col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# combine both data frames to append `1900-01-01` to all other dates\n",
    "df_st_date_departures = df_st_date_departures_default.union(df_st_date_departures_min_max)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_st_date_departures.printSchema()\n",
    "df_st_date_departures.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% check entries - beginning\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_st_date_departures.tail(5)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% check entries - ending\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Append date specific columns to staging table `st_date_departures`.\n",
    "# create new columns of st_date_departures table\n",
    "# https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "\n",
    "df_st_date_departures = df_st_date_departures \\\n",
    "    .withColumn(\"st_dd_id\", col(\"st_dd_date\")) \\\n",
    "    .withColumn(\"st_dd_year\", F.year(col(\"st_dd_date\"))) \\\n",
    "    .withColumn(\"st_dd_year_quarter\", F.concat_ws('/', F.year(col(\"st_dd_date\")), F.quarter(col(\"st_dd_date\")))) \\\n",
    "    .withColumn(\"st_dd_year_month\", F.concat_ws('/', F.year(col(\"st_dd_date\")), date_format(col(\"st_dd_date\"), \"MM\")) )\\\n",
    "    .withColumn(\"st_dd_quarter\", F.quarter(col(\"st_dd_date\"))) \\\n",
    "    .withColumn(\"st_dd_month\", F.month(\"st_dd_date\")) \\\n",
    "    .withColumn(\"st_dd_week\", F.weekofyear(col(\"st_dd_date\"))) \\\n",
    "    .withColumn(\"st_dd_weekday\", F.date_format(col(\"st_dd_date\"),'EEEE')) \\\n",
    "    .withColumn(\"st_dd_weekday_short\", F.date_format(col(\"st_dd_date\"),'EEE')) \\\n",
    "    .withColumn(\"st_dd_dayofweek\", F.dayofweek(col(\"st_dd_date\"))) \\\n",
    "    .withColumn(\"st_dd_day\", F.dayofmonth(col(\"st_dd_date\")) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get prepared staging table\n",
    "df_st_date_departures.printSchema()\n",
    "df_st_date_departures.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# persist staging time table 'st_date_departures'\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_stage/PQ3/st_date_deaprtures\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_st_date_departures \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create dimension 'd_date_arrivals' from staging table 'st_date_arrivals'\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_stage/PQ3/st_date_deaprtures\"\n",
    "df_st_date_departures = spark.read.parquet(location_to_read)\n",
    "\n",
    "print(df_st_date_departures.count())\n",
    "df_st_date_departures.printSchema()\n",
    "\n",
    "\n",
    "df_st_date_departures = df_st_date_departures \\\n",
    "    .withColumnRenamed(\"st_dd_date\", \"d_dd_date\") \\\n",
    "    .withColumnRenamed(\"st_dd_id\", \"d_dd_id\") \\\n",
    "    .withColumnRenamed(\"st_dd_year\", \"d_dd_year\") \\\n",
    "    .withColumnRenamed(\"st_dd_year_quarter\", \"d_dd_year_quarter\") \\\n",
    "    .withColumnRenamed(\"st_dd_year_month\", \"d_dd_year_month\") \\\n",
    "    .withColumnRenamed(\"st_dd_quarter\", \"d_dd_quarter\") \\\n",
    "    .withColumnRenamed(\"st_dd_month\", \"d_dd_month\") \\\n",
    "    .withColumnRenamed(\"st_dd_week\", \"d_dd_week\") \\\n",
    "    .withColumnRenamed(\"st_dd_weekday\", \"d_dd_weekday\") \\\n",
    "    .withColumnRenamed(\"st_dd_weekday_short\", \"d_dd_weekday_short\") \\\n",
    "    .withColumnRenamed(\"st_dd_dayofweek\", \"d_dd_dayofweek\") \\\n",
    "    .withColumnRenamed(\"st_dd_day\", \"d_dd_day\") \\\n",
    "\n",
    "df_st_date_departures.printSchema()\n",
    "df_st_date_departures.show(5)\n",
    "\n",
    "\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_star/PQ3/d_date_departures\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_st_date_departures \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Map dimension `d_date_arrivals` to  fact table `f_i94_immigration` based on columns\n",
    "   (`st_date_arrivals.st_da_date` --> `d_date_arrivals.d_da_id`) == (`st_i94_immigration.st_i94_arrdate_iso` --> `f_i94_immigration.d_da_id`).\n",
    "\n",
    "7. Map dimension `d_date_departures` to  fact table `f_i94_immigration` based on columns\n",
    "   (`st_date_departures.st_dd_date` --> `d_date_departures.d_dd_id`) == (`st_i94_immigration.st_i94_depdate_iso` --> `f_i94_immigration.d_dd_id`).\n",
    "\n",
    "8. Answer Project Question 3: At what times do foreign persons arrive for immigration to the U.S.?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reload fact table\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ3/f_i94_immigrations\"\n",
    "df_f_i94_immigrations = spark.read.parquet(location_to_read)\n",
    "print(df_f_i94_immigrations.count())\n",
    "df_f_i94_immigrations.printSchema()\n",
    "df_f_i94_immigrations.show(5, False)\n",
    "\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ3/d_date_arrivals\"\n",
    "df_d_date_arrivals = spark.read.parquet(location_to_read)\n",
    "print(df_d_date_arrivals.count())\n",
    "df_d_date_arrivals.printSchema()\n",
    "df_d_date_arrivals.show(5, False)\n",
    "\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ3/d_date_departures\"\n",
    "df_d_date_departures = spark.read.parquet(location_to_read)\n",
    "print(df_d_date_departures.count())\n",
    "df_d_date_departures.printSchema()\n",
    "df_d_date_departures.show(5, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Register data frames as Views\n",
    "df_f_i94_immigrations.createOrReplaceTempView(\"f_i94_immigrations\")\n",
    "df_d_date_arrivals.createOrReplaceTempView(\"d_date_arrivals\")\n",
    "df_d_date_departures.createOrReplaceTempView(\"d_date_departures\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. Answer Project Question 3.1: At what times do foreign persons arrive for immigration to the U.S.?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SQL to answer Project Question 3.1: At what times do foreign persons arrive for immigration to the U.S.?\n",
    "df_pq3_1 = spark.sql(\"select da.d_da_year_month as Year_Month\"\n",
    "                     \"      ,count(f_i94.f_i94_count) as  Immigrants\"\n",
    "                     \"      ,RANK() OVER (ORDER BY count(f_i94.f_i94_count) desc) Immigrants_rank\"\n",
    "                     \"  from f_i94_immigrations f_i94\"\n",
    "                     \"  join d_date_arrivals da on da.d_da_id = f_i94.d_da_id  \"\n",
    "                     \" group by Year_Month \"\n",
    "                     \" order by Year_Month  \"\n",
    "                     )\n",
    "\n",
    "df_pq3_1.show(5000, False)\n",
    "\n",
    "df_pq3_11 = spark.sql(\"select da.d_da_year_month as Year_Month\"\n",
    "                     \"      ,count(f_i94.f_i94_count) as  Immigrants\"\n",
    "                     \"      ,RANK() OVER (ORDER BY count(f_i94.f_i94_count) desc) Immigrants_rank\"\n",
    "                     \"  from f_i94_immigrations f_i94\"\n",
    "                     \"  join d_date_arrivals da on da.d_da_id = f_i94.d_da_id  \"\n",
    "                     \" group by Year_Month \"\n",
    "                     \" order by Immigrants_rank  \"\n",
    "                     )\n",
    "\n",
    "df_pq3_11.show(5000, False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. Answer Project Question 3.2: When a foreign person comes to the U.S. for immigration, do they travel on to\n",
    "   another state?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SQL to answer Project Question 3.2: When a foreign person comes to the U.S. for immigration, do they travel on to\n",
    "# another state?\n",
    "df_pq3_2 = spark.sql(\"select da.d_da_year_month as Year_Month_arrival\"\n",
    "                     \"      ,dd.d_dd_year_month as Year_Month_dearture\"\n",
    "                     \"      ,count(f_i94.f_i94_count) as Immigrants \"\n",
    "                     \"  from f_i94_immigrations f_i94\"\n",
    "                     \"  join d_date_arrivals da on da.d_da_id = f_i94.d_da_id  \"\n",
    "                     \" left join d_date_departures dd on dd.d_dd_id = f_i94.d_dd_id  \"\n",
    "                     \" group by Year_Month_arrival, Year_Month_dearture\"\n",
    "                     \" order by Year_Month_arrival, Year_Month_dearture, Immigrants\"\n",
    "                     )\n",
    "\n",
    "df_pq3_2.show(5000, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pq3_2 = spark.sql(\"select da.d_da_year_month as Year_Month_arrival\"\n",
    "                     \"      ,dd.d_dd_year_month as Year_Month_dearture\"\n",
    "                     \"      ,count(f_i94.f_i94_count) as Immigrants \"\n",
    "                     \"  from f_i94_immigrations f_i94\"\n",
    "                     \"  join d_date_arrivals da on da.d_da_id = f_i94.d_da_id  \"\n",
    "                     \" left join d_date_departures dd on dd.d_dd_id = f_i94.d_dd_id  \"\n",
    "                     \" group by Year_Month_arrival, Year_Month_dearture\"\n",
    "                     \" order by Immigrants desc \"\n",
    "                     )\n",
    "\n",
    "df_pq3_2.show(5000, False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10. Answer Project Question 3.3: If a foreign person travels to another state after immigration. After which period of\n",
    "    time does this happen?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DF to answer Project Question 3.3: If a foreign person travels to another state, after which period of time does this happen?\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "windowSpec  = Window.orderBy(col(\"immigrants\").desc())\n",
    "\n",
    "df_f_i94_immigrations \\\n",
    "    .join(df_d_date_arrivals , df_f_i94_immigrations.d_da_id == df_d_date_arrivals.d_da_id) \\\n",
    "    .join(df_d_date_departures, df_f_i94_immigrations.d_dd_id == df_d_date_departures.d_dd_id) \\\n",
    "    .filter(\"f_i94_depdate_iso != '1900-01-01'\") \\\n",
    "    .withColumn(\"departure_days_after_arrival\", F.datediff(col(\"f_i94_depdate_iso\"), col(\"f_i94_arrdate_iso\"))) \\\n",
    "    .select( \"d_da_date\"\n",
    "            ,\"d_dd_date\"\n",
    "            ,\"departure_days_after_arrival\") \\\n",
    "    .groupBy(\"departure_days_after_arrival\").count() \\\n",
    "    .withColumnRenamed(\"count\", \"immigrants\") \\\n",
    "    .withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show(500)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 4.1.4. To which states in the U.S. do immigrants want to continue their travel after their initial arrival and what demographics can immigrants expect when they arrive in the destination state, such as average temperature, population numbers or population density? [(Data description)](#question4_description) <a name=\"question4_data_pipeline\">\n",
    "1. Clean data and create staging table `st_state_destinations` from file\n",
    "   [I94_SAS_Labels_I94ADDR.txt](../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94ADDR.txt)\n",
    "   based on columns `st_sd_state_code` and `st_sd_state_name`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get data\n",
    "location_to_read = \"../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94ADDR.txt\"\n",
    "df_st_I94_SAS_Labels_I94ADDR = spark.read.text(location_to_read)\n",
    "\n",
    "df_st_I94_SAS_Labels_I94ADDR.printSchema()\n",
    "df_st_I94_SAS_Labels_I94ADDR.show(5, False)\n",
    "\n",
    "# get regex_cleaned values -->\n",
    "regex_cleaned = r\"^\\s+'([9+A-Z]+)'='([A-Z\\s.]+)'\"\n",
    "\n",
    "df_st_I94_SAS_Labels_I94ADDR_regex_cleaned = df_st_I94_SAS_Labels_I94ADDR \\\n",
    "    .select( F.regexp_extract('value',regex_cleaned, 1).alias('st_sd_state_code'),\n",
    "             F.regexp_extract('value',regex_cleaned, 2).alias('st_sd_state_name')) \\\n",
    "    .drop_duplicates() \\\n",
    "    .orderBy(\"st_sd_state_code\")\n",
    "\n",
    "print(df_st_I94_SAS_Labels_I94ADDR_regex_cleaned.count())\n",
    "df_st_I94_SAS_Labels_I94ADDR_regex_cleaned.show(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This step is optional\n",
    "location_to_write = \"../P8_capstone_resource_files/I94_sas_labels_descriptions_extracted_data/I94_SAS_Labels_I94ADDR.csv\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_st_I94_SAS_Labels_I94ADDR_regex_cleaned\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(location_to_write, mode='overwrite')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Extract some demographic data from file [us-cities-demographics.json](../P8_capstone_resource_files/us-cities-demographics.json)\n",
    "   like `age_median`, `population_male`, `population_female`, `population_total` or `foreign_born` and add them to staging\n",
    "   table `st_state_destinations`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get data from JSON-file\n",
    "location_to_read = \"../P8_capstone_resource_files/us-cities-demographics.json\"\n",
    "df_us_cities_demographics = spark.read.json(location_to_read)\n",
    "\n",
    "print(df_us_cities_demographics.count())\n",
    "df_us_cities_demographics.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Check data for further processing\n",
    "df_us_cities_demographics \\\n",
    "    .filter(\"fields.state == 'Alabama'\") \\\n",
    "    .select(\"fields.state_code\"\n",
    "            , \"fields.state\"\n",
    "            , \"fields.city\"\n",
    "            , \"fields.median_age\"\n",
    "            , \"fields.male_population\"\n",
    "            , \"fields.female_population\"\n",
    "            , \"fields.total_population\"\n",
    "            , \"fields.foreign_born\") \\\n",
    "    .distinct()\\\n",
    "    .orderBy(\"fields.state_code\")\\\n",
    "    .show(50)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get only values aggregated by state and not the city values.\n",
    "df_us_cities_demographics_agg = df_us_cities_demographics \\\n",
    "    .groupBy(\"fields.state_code\", \"fields.state\") \\\n",
    "    .agg(  F.round(F.avg('fields.median_age'),1).alias('st_sd_age_median')\n",
    "          ,F.round(F.avg('fields.male_population').cast(IntegerType()),2).alias('st_sd_population_male')\n",
    "          ,F.round(F.avg('fields.female_population').cast(IntegerType()),2).alias('st_sd_population_female')\n",
    "          ,F.round(F.avg('fields.total_population').cast(IntegerType()),2).alias('st_sd_population_total')\n",
    "          ,F.round(F.avg('fields.foreign_born').cast(IntegerType()),2).alias('st_sd_foreign_born')\n",
    "           ) \\\n",
    "    .orderBy(\"fields.state_code\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df_us_cities_demographics_agg.count())\n",
    "df_us_cities_demographics_agg.printSchema()\n",
    "df_us_cities_demographics_agg.show(500)\n",
    "\n",
    "# Join \"df_st_I94_SAS_Labels_I94ADDR_regex_cleaned\" and \"df_us_cities_demographics\" to get new data frame \"df_st_state_destinations\"\n",
    "# fill up null values with 0\n",
    "df_st_state_destinations = df_st_I94_SAS_Labels_I94ADDR_regex_cleaned \\\n",
    "    .join(df_us_cities_demographics_agg, df_st_I94_SAS_Labels_I94ADDR_regex_cleaned.st_sd_state_code ==\n",
    "          df_us_cities_demographics_agg.fields.state_code, 'left'  )\\\n",
    "    .drop(\"state_code\", \"state\") \\\n",
    "    .withColumn(\"st_sd_state_name\", F.initcap(col(\"st_sd_state_name\"))) \\\n",
    "    .fillna(value=0.0 ,subset=['st_sd_age_median'])\\\n",
    "    .fillna(value=0 ,subset=['st_sd_population_male'])\\\n",
    "    .fillna(value=0 ,subset=['st_sd_population_female'])\\\n",
    "    .fillna(value=0 ,subset=['st_sd_population_total'])\\\n",
    "    .fillna(value=0 ,subset=['st_sd_foreign_born'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_st_state_destinations = df_st_I94_SAS_Labels_I94ADDR_regex_cleaned \\\n",
    "    .join(df_us_cities_demographics, df_st_I94_SAS_Labels_I94ADDR_regex_cleaned.st_sd_state_code ==\n",
    "          df_us_cities_demographics.fields.state_code, 'left'  )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_st_state_destinations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check results\n",
    "print(df_st_state_destinations.count())\n",
    "df_st_state_destinations.printSchema()\n",
    "df_st_state_destinations\\\n",
    "    .orderBy(\"st_sd_state_code\") \\\n",
    "    .show(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_st_state_destinations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check results\n",
    "print(df_st_state_destinations.count())\n",
    "df_st_state_destinations.printSchema()\n",
    "df_st_state_destinations\\\n",
    "    .orderBy(\"st_sd_state_code\") \\\n",
    "    .show(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store staging table\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_stage/PQ4/st_state_destinations\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_st_state_destinations \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Creation of a dimension named `d_state_destinations` based on staging table `st_state_destinations`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get data to process and store dimension \"d_state_destinations\"\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_stage/PQ4/st_state_destinations\"\n",
    "df_st_state_destinations = spark.read.parquet(location_to_read)\n",
    "\n",
    "print(df_st_state_destinations.count())\n",
    "df_st_state_destinations.printSchema()\n",
    "\n",
    "df_st_state_destinations = df_st_state_destinations \\\n",
    "    .withColumn(\"d_sd_id\", col(\"st_sd_state_code\")) \\\n",
    "    .withColumnRenamed(\"st_sd_state_code\", \"d_sd_state_code\") \\\n",
    "    .withColumnRenamed(\"st_sd_state_name\", \"d_sd_state_name\") \\\n",
    "    .withColumnRenamed(\"st_sd_age_median\", \"d_sd_age_median\") \\\n",
    "    .withColumnRenamed(\"st_sd_population_male\", \"d_sd_population_male\") \\\n",
    "    .withColumnRenamed(\"st_sd_population_female\", \"d_sd_population_female\") \\\n",
    "    .withColumnRenamed(\"st_sd_population_total\", \"d_sd_population_total\") \\\n",
    "    .withColumnRenamed(\"st_sd_foreign_born\", \"d_sd_foreign_born\") \\\n",
    "\n",
    "df_st_state_destinations.printSchema()\n",
    "\n",
    "# store dimension table\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_star/PQ4/d_state_destinations\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_st_state_destinations \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")\n",
    "\n",
    "df_st_state_destinations.orderBy(\"d_sd_state_code\").show(1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Mapping of dimension `d_state_destinations` to  fact table `f_i94_immigration` based on columns\n",
    "   (`st_state_destinations.st_sd_state_code` --> `d_state_destinations.d_sd_id`) ==\n",
    "   (`st_i94_immigration.st_i94_addr` --> `f_i94_immigration.d_sd_id`)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Clean fact table `f_i94_immigration` based on the dimension `d_state_destinations`. All unrecognizable columns will\n",
    "be set to 99 (all other countries)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Get data for further processing\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ3/f_i94_immigrations\"\n",
    "df_f_i94_immigrations = spark.read.parquet(location_to_read)\n",
    "\n",
    "print(df_f_i94_immigrations.count())\n",
    "df_f_i94_immigrations.printSchema()\n",
    "\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ4/d_state_destinations\"\n",
    "df_d_state_destinations = spark.read.parquet(location_to_read)\n",
    "\n",
    "print(df_d_state_destinations.count())\n",
    "df_d_state_destinations.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare data frame `df_f_i94_immigrations_2_join` to get only the allowed state codes\n",
    "df_f_i94_immigrations_2_join = df_d_state_destinations \\\n",
    "    .select(\"d_sd_id\")\\\n",
    "    .withColumnRenamed(\"d_sd_id\", \"d_sd_id_reference\") \\\n",
    "    .orderBy(\"d_sd_id_reference\")\n",
    "\n",
    "print(df_f_i94_immigrations_2_join.count())\n",
    "df_f_i94_immigrations_2_join.printSchema()\n",
    "df_f_i94_immigrations_2_join.show(60)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare and create a cleaned column \"d_sd_id_cleaned\"\n",
    "df_f_i94_immigrations \\\n",
    "    .select(\"d_sd_id\", \"f_i94_addr\") \\\n",
    "    .join(df_f_i94_immigrations_2_join, df_f_i94_immigrations_2_join.d_sd_id_reference == df_f_i94_immigrations.d_sd_id, 'left') \\\n",
    "    .withColumn(\"d_sd_id_cleaned\", when(col(\"d_sd_id_reference\").isNull(), \"99\")\\\n",
    "                .otherwise(col(\"d_sd_id_reference\"))) \\\n",
    "    .filter(col(\"d_sd_id_reference\").isNull())\\\n",
    "    .distinct() \\\n",
    "    .orderBy(\"d_sd_id\") \\\n",
    "    .show(5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# clean column \"f_i94_immigrations.d_sd_id\" by column \"d_sd_id_cleaned (d_sd_id_reference)\"\n",
    "df_f_i94_immigrations = df_f_i94_immigrations \\\n",
    "    .join(df_f_i94_immigrations_2_join, df_f_i94_immigrations_2_join.d_sd_id_reference == df_f_i94_immigrations.d_sd_id, 'left') \\\n",
    "    .withColumn(\"d_sd_id\", when(col(\"d_sd_id_reference\").isNull(), \"99\")\\\n",
    "                .otherwise(col(\"d_sd_id_reference\"))) \\\n",
    "    .drop(\"d_sd_id_reference\") \\"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check corrected column \"d_sd_id\"\n",
    "df_f_i94_immigrations \\\n",
    "    .select(\"d_sd_id\", \"f_i94_addr\") \\\n",
    "    .distinct() \\\n",
    "    .orderBy(\"d_sd_id\", \"f_i94_addr\") \\\n",
    "    .show(5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df_f_i94_immigrations.count())\n",
    "df_f_i94_immigrations.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write fact table f_i94_immigration (~ 137,7 MB)\n",
    "location_to_write = \"../P8_capstone_resource_files/parquet_star/PQ4/f_i94_immigrations\"\n",
    "\n",
    "# delete folder if already exists\n",
    "if path.exists(location_to_write):\n",
    "    shutil.rmtree(location_to_write)\n",
    "\n",
    "df_f_i94_immigrations \\\n",
    "    .repartition(int(1)) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(saveMode='overwrite') \\\n",
    "    .partitionBy('f_i94_year', 'f_i94_month') \\\n",
    "    .parquet(location_to_write, compression=\"gzip\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Answer Project Question 4: To which states in the U.S. do immigrants want to continue their travel after their initial\n",
    "   arrival and what demographics can immigrants expect when they arrive in the destination state, such as average\n",
    "   temperature, population numbers or population density?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Get data for further processing\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ4/f_i94_immigrations\"\n",
    "df_f_i94_immigrations = spark.read.parquet(location_to_read)\n",
    "\n",
    "print(df_f_i94_immigrations.count())\n",
    "df_f_i94_immigrations.printSchema()\n",
    "\n",
    "\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ4/d_state_destinations\"\n",
    "df_d_state_destinations = spark.read.parquet(location_to_read)\n",
    "\n",
    "print(df_d_state_destinations.count())\n",
    "df_d_state_destinations.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Register data frames as Views\n",
    "df_f_i94_immigrations.createOrReplaceTempView(\"f_i94_immigrations\")\n",
    "df_d_state_destinations.createOrReplaceTempView(\"d_state_destinations\")\n",
    "\n",
    "\n",
    "# Answer Project question #6: The Answer is \"California\"\n",
    "df_pq4 = spark.sql(\" select \"\n",
    "                   \"        RANK() OVER (ORDER BY count(f_i94.f_i94_count) desc) immigrants_continue_travel_rank\"\n",
    "                   \"       ,d_sd.d_sd_state_code as state_code\"\n",
    "                   \"       ,d_sd.d_sd_state_name as state_name\"\n",
    "                   \"       ,count(f_i94.f_i94_count) as immigrants_continue_travel \"\n",
    "                   \"       ,d_sd.d_sd_age_median as age_median\"\n",
    "                   \"       ,d_sd.d_sd_population_male as population_male\"\n",
    "                   \"       ,d_sd.d_sd_population_female as population_female\"\n",
    "                   \"       ,d_sd.d_sd_population_total as population_total\"\n",
    "                   \"       ,d_sd.d_sd_foreign_born as foreign_born\"\n",
    "                   \" from f_i94_immigrations f_i94\"\n",
    "                   \" join d_state_destinations d_sd on d_sd.d_sd_id == f_i94.d_sd_id\"\n",
    "                   \" group by state_code\"\n",
    "                   \"         ,state_name\"\n",
    "                   \"         ,age_median\"\n",
    "                   \"         ,population_male\"\n",
    "                   \"         ,population_female\"\n",
    "                   \"         ,population_total\"\n",
    "                   \"         ,foreign_born\"\n",
    "                   \" order by immigrants_continue_travel desc \")\n",
    "\n",
    "df_pq4.show(500)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    "\n",
    "Run Quality Checks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 4.2.1 Define StructType and create result data frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define format to store data quality result data frame\n",
    "result_struct_type = StructType(\n",
    "    [\n",
    "         StructField(\"dq_result_table_name\", StringType(), True)\n",
    "        ,StructField(\"dq_result_null_entries\", IntegerType(), True)\n",
    "        ,StructField(\"dq_result_entries\", IntegerType(), True)\n",
    "        ,StructField(\"dq_result_status\", StringType(), True)\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# execute check commands"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#####  4.2.2 Data Quality (dq) checks for table d_immigration_countries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read table table\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ1/d_immigration_countries\"\n",
    "df_dq_table_d_immigration_countries = spark.read.parquet(location_to_read)\n",
    "\n",
    "# Check if key fields have valid values (no nulls or empty)\n",
    "df_dq_check_null_values = df_dq_table_d_immigration_countries \\\n",
    "    .select(\"d_ic_id\") \\\n",
    "    .where(\"d_ic_id is null or d_ic_id == ''\") \\\n",
    "    .count()\n",
    "\n",
    "print(f\"df_dq_check_null_values: {df_dq_check_null_values}\")\n",
    "\n",
    "# Check that table has > 0 rows\n",
    "df_dq_check_content = df_dq_table_d_immigration_countries.count()\n",
    "print(f\"df_dq_check_content: {df_dq_check_content}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# insert result into result_df\n",
    "table_name = \"d_immigration_countries\"\n",
    "if df_dq_check_null_values < 1 and df_dq_check_content > 0:\n",
    "    dq_check_result = \"OK\"\n",
    "else:\n",
    "    dq_check_result = \"NOK\"\n",
    "\n",
    "print(dq_check_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dq_results = [\n",
    "    (table_name, df_dq_check_null_values, df_dq_check_content, dq_check_result)\n",
    "]\n",
    "print(dq_results)\n",
    "\n",
    "# create results data frame\n",
    "df_dq_results = spark.createDataFrame(dq_results, result_struct_type)\n",
    "\n",
    "# check df schema and content\n",
    "df_dq_results.printSchema()\n",
    "df_dq_results.show(100, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#####  4.2.3 Data Quality (dq) checks for table d_immigration_airports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read table table\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ2/d_immigration_airports\"\n",
    "df_dq_table_d_immigration_airports = spark.read.parquet(location_to_read)\n",
    "\n",
    "# Check if key fields have valid values (no nulls or empty)\n",
    "df_dq_check_null_values = df_dq_table_d_immigration_airports \\\n",
    "    .select(\"d_ia_id\") \\\n",
    "    .where(\"d_ia_id is null or d_ia_id == ''\") \\\n",
    "    .count()\n",
    "\n",
    "# Check that table has > 0 rows\n",
    "df_dq_check_content = df_dq_table_d_immigration_airports.count()\n",
    "\n",
    "print(f\"df_dq_check_null_values: {df_dq_check_null_values}\")\n",
    "print(f\"df_dq_check_content: {df_dq_check_content}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# insert result into result_df\n",
    "table_name = \"d_immigration_airports\"\n",
    "if df_dq_check_null_values < 1 and df_dq_check_content > 0:\n",
    "    dq_check_result = \"OK\"\n",
    "else:\n",
    "    dq_check_result = \"NOK\"\n",
    "\n",
    "print(f\"table_name: {table_name}\")\n",
    "print(f\"dq_check_result: {dq_check_result}\")\n",
    "print(f\"df_dq_check_null_values: {df_dq_check_null_values}\")\n",
    "print(f\"df_dq_check_content: {df_dq_check_content}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dq_results = [\n",
    "    (table_name, df_dq_check_null_values, df_dq_check_content, dq_check_result)\n",
    "]\n",
    "print(dq_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add new row to current results data frame\n",
    "new_row = spark.createDataFrame(dq_results, result_struct_type)\n",
    "df_dq_results = df_dq_results.union(new_row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_dq_results.show(10, False)\n",
    "\n",
    "##------------------------------------------------------------------------#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#####  4.2.4 Data Quality (dq) checks for table d_date_arrivals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read table table\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ3/d_date_arrivals\"\n",
    "df_dq_table_d_date_arrivals = spark.read.parquet(location_to_read)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if key fields have valid values (no nulls or empty)\n",
    "df_dq_check_null_values = df_dq_table_d_date_arrivals \\\n",
    "    .select(\"d_da_id\") \\\n",
    "    .where(\"d_da_id is null or d_da_id == ''\") \\\n",
    "    .count()\n",
    "\n",
    "# Check that table has > 0 rows\n",
    "df_dq_check_content = df_dq_table_d_date_arrivals.count()\n",
    "\n",
    "print(f\"df_dq_check_null_values: {df_dq_check_null_values}\")\n",
    "print(f\"df_dq_check_content: {df_dq_check_content}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# insert result into result_df\n",
    "table_name = \"d_date_arrivals\"\n",
    "if df_dq_check_null_values < 1 and df_dq_check_content > 0:\n",
    "    dq_check_result = \"OK\"\n",
    "else:\n",
    "    dq_check_result = \"NOK\"\n",
    "\n",
    "print(f\"table_name: {table_name}\")\n",
    "print(f\"dq_check_result: {dq_check_result}\")\n",
    "print(f\"df_dq_check_null_values: {df_dq_check_null_values}\")\n",
    "print(f\"df_dq_check_content: {df_dq_check_content}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dq_results = [\n",
    "    (table_name, df_dq_check_null_values, df_dq_check_content, dq_check_result)\n",
    "]\n",
    "print(dq_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add new row to current results data frame\n",
    "new_row = spark.createDataFrame(dq_results, result_struct_type)\n",
    "df_dq_results = df_dq_results.union(new_row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_dq_results.show(10, False)\n",
    "\n",
    "##------------------------------------------------------------------------#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#####  4.2.5 Data Quality (dq) checks for table d_date_departures"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read table table\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ3/d_date_departures\"\n",
    "df_dq_table_d_date_departures = spark.read.parquet(location_to_read)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if key fields have valid values (no nulls or empty)\n",
    "df_dq_check_null_values = df_dq_table_d_date_departures \\\n",
    "    .select(\"d_dd_id\") \\\n",
    "    .where(\"d_dd_id is null or d_dd_id == ''\") \\\n",
    "    .count()\n",
    "\n",
    "# Check that table has > 0 rows\n",
    "df_dq_check_content = df_dq_table_d_date_departures.count()\n",
    "\n",
    "print(f\"df_dq_check_null_values: {df_dq_check_null_values}\")\n",
    "print(f\"df_dq_check_content: {df_dq_check_content}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# insert result into result_df\n",
    "table_name = \"d_date_departures\"\n",
    "if df_dq_check_null_values < 1 and df_dq_check_content > 0:\n",
    "    dq_check_result = \"OK\"\n",
    "else:\n",
    "    dq_check_result = \"NOK\"\n",
    "\n",
    "print(f\"table_name: {table_name}\")\n",
    "print(f\"dq_check_result: {dq_check_result}\")\n",
    "print(f\"df_dq_check_null_values: {df_dq_check_null_values}\")\n",
    "print(f\"df_dq_check_content: {df_dq_check_content}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dq_results = [\n",
    "    (table_name, df_dq_check_null_values, df_dq_check_content, dq_check_result)\n",
    "]\n",
    "print(dq_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add new row to current results data frame\n",
    "new_row = spark.createDataFrame(dq_results, result_struct_type)\n",
    "df_dq_results = df_dq_results.union(new_row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_dq_results.show(10, False)\n",
    "\n",
    "##------------------------------------------------------------------------#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#####  4.2.6 Data Quality (dq) checks for table d_state_destinations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read table table\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ4/d_state_destinations\"\n",
    "df_dq_table_d_state_destinations = spark.read.parquet(location_to_read)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if key fields have valid values (no nulls or empty)\n",
    "df_dq_check_null_values = df_dq_table_d_state_destinations \\\n",
    "    .select(\"d_sd_id\") \\\n",
    "    .where(\"d_sd_id is null or d_sd_id == ''\") \\\n",
    "    .count()\n",
    "\n",
    "# Check that table has > 0 rows\n",
    "df_dq_check_content = df_dq_table_d_state_destinations.count()\n",
    "\n",
    "print(f\"df_dq_check_null_values: {df_dq_check_null_values}\")\n",
    "print(f\"df_dq_check_content: {df_dq_check_content}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# insert result into result_df\n",
    "table_name = \"d_state_destinations\"\n",
    "if df_dq_check_null_values < 1 and df_dq_check_content > 0:\n",
    "    dq_check_result = \"OK\"\n",
    "else:\n",
    "    dq_check_result = \"NOK\"\n",
    "\n",
    "print(f\"table_name: {table_name}\")\n",
    "print(f\"dq_check_result: {dq_check_result}\")\n",
    "print(f\"df_dq_check_null_values: {df_dq_check_null_values}\")\n",
    "print(f\"df_dq_check_content: {df_dq_check_content}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dq_results = [\n",
    "    (table_name, df_dq_check_null_values, df_dq_check_content, dq_check_result)\n",
    "]\n",
    "print(dq_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add new row to current results data frame\n",
    "new_row = spark.createDataFrame(dq_results, result_struct_type)\n",
    "df_dq_results = df_dq_results.union(new_row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_dq_results.show(10, False)\n",
    "\n",
    "##------------------------------------------------------------------------#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#####  4.2.7 Data Quality (dq) checks for table f_i94_immigrations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read table table\n",
    "location_to_read = \"../P8_capstone_resource_files/parquet_star/PQ4/f_i94_immigrations\"\n",
    "df_dq_table_f_i94_immigrations = spark.read.parquet(location_to_read)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if key fields have valid values (no nulls or empty)\n",
    "df_dq_check_null_values = df_dq_table_f_i94_immigrations \\\n",
    "    .select(  \"f_i94_id\"\n",
    "            , \"d_ia_id\"\n",
    "            , \"d_sd_id\"\n",
    "            , \"d_da_id\"\n",
    "            , \"d_dd_id\"\n",
    "            , \"d_ic_id\"\n",
    "            ) \\\n",
    "    .where(  \"    f_i94_id is null or f_i94_id == ''\"\n",
    "             \" or d_ia_id is null or d_ia_id == ''\"\n",
    "             \" or d_sd_id is null or d_sd_id == ''\"\n",
    "             \" or d_da_id is null or d_da_id == ''\"\n",
    "             \" or d_dd_id is null or d_dd_id == ''\"\n",
    "             \" or d_ic_id is null or d_ic_id == ''\") \\\n",
    "    .count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check that table has > 0 rows\n",
    "df_dq_check_content = df_dq_table_f_i94_immigrations.count()\n",
    "\n",
    "print(f\"df_dq_check_null_values: {df_dq_check_null_values}\")\n",
    "print(f\"df_dq_check_content: {df_dq_check_content}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# insert result into result_df\n",
    "table_name = \"f_i94_immigrations\"\n",
    "if df_dq_check_null_values < 1 and df_dq_check_content > 0:\n",
    "    dq_check_result = \"OK\"\n",
    "else:\n",
    "    dq_check_result = \"NOK\"\n",
    "\n",
    "print(f\"table_name: {table_name}\")\n",
    "print(f\"dq_check_result: {dq_check_result}\")\n",
    "print(f\"df_dq_check_null_values: {df_dq_check_null_values}\")\n",
    "print(f\"df_dq_check_content: {df_dq_check_content}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dq_results = [\n",
    "    (table_name, df_dq_check_null_values, df_dq_check_content, dq_check_result)\n",
    "]\n",
    "print(dq_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add new row to current results data frame\n",
    "new_row = spark.createDataFrame(dq_results, result_struct_type)\n",
    "df_dq_results = df_dq_results.union(new_row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_dq_results.show(10, False)\n",
    "##------------------------------------------------------------------------#\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.3 Data dictionary\n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where\n",
    "it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "Generate a dictionary from table columns to fill out manually the meaning of the current column like the following example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create data dictionary of used star schema\n",
    "def create_data_dictionary_from_df(location_to_read):\n",
    "    json_table_data = {}\n",
    "    tables = {}\n",
    "\n",
    "    # loop thru list of data frames to read (star schema)\n",
    "    for current_table_name_from_df in locations_to_read:\n",
    "\n",
    "        location_to_read = current_table_name_from_df\n",
    "        regex = r\"^.*\\/(\\w+)$\"\n",
    "        matches = re.finditer(regex, location_to_read, re.MULTILINE)\n",
    "\n",
    "        # get table name from location string of source data\n",
    "        for matchNum, match in enumerate(matches, start=1):\n",
    "            current_table = match.group(matchNum)\n",
    "\n",
    "        # Set current table name. Table description will be filled in later. Columns will be appended later also.\n",
    "        dict_current_table = {\"table_name\": current_table,\n",
    "                              \"table_description\": \"not set\"}\n",
    "\n",
    "        # read all table columns for current table from df\n",
    "        current_table_columns_df = [spark.read.parquet(location_to_read).columns]\n",
    "\n",
    "        # create dictionary from table columns\n",
    "        current_table_columns_dict = {}\n",
    "\n",
    "        # loop thru list \"current_table_columns_df\" and add columns to dict \"current_table_columns_dict\"\n",
    "        for counter, current_table_columns_df_column in enumerate(current_table_columns_df, start=1):\n",
    "            for current_column in enumerate(current_table_columns_df_column, start=1):\n",
    "                current_table_column_name_dict = {\"column_name\": current_column[counter],\n",
    "                                                  \"column_description\": \"not set\"}\n",
    "                current_table_columns_dict[current_column[counter]] = current_table_column_name_dict\n",
    "\n",
    "        dict_current_table[\"columns\"] = current_table_columns_dict\n",
    "\n",
    "        tables[current_table] = dict_current_table\n",
    "\n",
    "        # add tables content to the dict json_data\n",
    "        json_table_data[\"tables\"] = tables\n",
    "\n",
    "    return json_table_data\n",
    "\n",
    "\n",
    "# add table and column descriptions manually\n",
    "def update_descriptions(json_data_dictionary):\n",
    "\n",
    "    # The following part is specific to this Project. Every description has to be configured separately\n",
    "\n",
    "    # Table d_immigration_countries\n",
    "    json_data_dictionary['tables']['d_immigration_countries']['table_description'] = \\\n",
    "        \"Country where immigrants come from to the U.S.\"\n",
    "    json_data_dictionary['tables']['d_immigration_countries']['columns']['d_ic_id']['column_description'] \\\n",
    "        = \"PK of table d_immigration_countries\"\n",
    "    json_data_dictionary['tables']['d_immigration_countries']['columns']['d_ic_country_code']['column_description'] \\\n",
    "        = \"Abbreviation of country code\"\n",
    "    json_data_dictionary['tables']['d_immigration_countries']['columns']['d_ic_country_name']['column_description'] \\\n",
    "        = \"Name of country\"\n",
    "\n",
    "    # Table d_immigration_airports\n",
    "    json_data_dictionary['tables']['d_immigration_airports']['table_description'] \\\n",
    "        = \"Airport name where foreign people arrive to the U.S. \"\n",
    "    json_data_dictionary['tables']['d_immigration_airports']['columns']['d_ia_id']['column_description'] \\\n",
    "        = \"PK of table d_immigration_airports\"\n",
    "    json_data_dictionary['tables']['d_immigration_airports']['columns']['d_ia_airport_code']['column_description'] \\\n",
    "        = \"Abbreviation code of Airport\"\n",
    "    json_data_dictionary['tables']['d_immigration_airports']['columns']['d_ia_airport_name']['column_description'] \\\n",
    "        = \"Name of Airport\"\n",
    "    json_data_dictionary['tables']['d_immigration_airports']['columns']['d_ia_airport_state_code']['column_description'] \\\n",
    "        = \"Abbreviation of state where Airport is located\"\n",
    "\n",
    "    # Table d_date_arrivals\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['table_description'] \\\n",
    "        = \"Arrival date for foreign persons to immigrate to the U.S.? \"\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['columns']['d_da_id']['column_description'] \\\n",
    "        = \"PK of table d_date_arrivals\"\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['columns']['d_da_date']['column_description'] \\\n",
    "        = \"Date when foreign persons arrive for immigration to the U.S. \"\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['columns']['d_da_year']['column_description'] \\\n",
    "        = \"Year of arrival like '2020'\"\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['columns']['d_da_year_quarter']['column_description'] \\\n",
    "        = \"Year and quarter of arrival like '2016/1'\"\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['columns']['d_da_year_month']['column_description'] \\\n",
    "        = \"Year and month of arrival like '2016/01'\"\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['columns']['d_da_quarter']['column_description'] \\\n",
    "        = \"Quarter of arrival like '1'\"\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['columns']['d_da_month']['column_description'] \\\n",
    "        = \"Month of arrival like '1'\"\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['columns']['d_da_week']['column_description'] \\\n",
    "        = \"Week of arrival like '53'\"\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['columns']['d_da_weekday']['column_description'] \\\n",
    "        = \"Day of week like 'Friday'\"\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['columns']['d_da_weekday_short']['column_description'] \\\n",
    "        = \"Day of week in short form like 'Fri'\"\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['columns']['d_da_dayofweek']['column_description'] \\\n",
    "        = \"Day of week as number like '6'\"\n",
    "    json_data_dictionary['tables']['d_date_arrivals']['columns']['d_da_day']['column_description'] \\\n",
    "        = \"Day number of current date like 2016-01-01 --> 1\"\n",
    "\n",
    "    # Table d_date_departures\n",
    "    json_data_dictionary['tables']['d_date_departures']['table_description'] = \"Departure date from USA\"\n",
    "    json_data_dictionary['tables']['d_date_departures']['columns']['d_dd_id']['column_description'] \\\n",
    "        = \"PK of table d_date_departures\"\n",
    "    json_data_dictionary['tables']['d_date_departures']['columns']['d_dd_date']['column_description'] \\\n",
    "        = \"Date when foreign persons departure for immigration to the U.S. \"\n",
    "    json_data_dictionary['tables']['d_date_departures']['columns']['d_dd_year']['column_description'] \\\n",
    "        = \"Year of departure like '2020'\"\n",
    "    json_data_dictionary['tables']['d_date_departures']['columns']['d_dd_year_quarter']['column_description'] \\\n",
    "        = \"Year and quarter of departure like '2016/1'\"\n",
    "    json_data_dictionary['tables']['d_date_departures']['columns']['d_dd_year_month']['column_description'] \\\n",
    "        = \"Year and quarter of departure like '2016/1'\"\n",
    "    json_data_dictionary['tables']['d_date_departures']['columns']['d_dd_quarter']['column_description'] \\\n",
    "        = \"Quarter of departure like '1'\"\n",
    "    json_data_dictionary['tables']['d_date_departures']['columns']['d_dd_month']['column_description'] \\\n",
    "        = \"Month of departure like '1'\"\n",
    "    json_data_dictionary['tables']['d_date_departures']['columns']['d_dd_week']['column_description'] \\\n",
    "        = \"Week of departure like '53'\"\n",
    "    json_data_dictionary['tables']['d_date_departures']['columns']['d_dd_weekday']['column_description'] \\\n",
    "        = \"Day of week like 'Friday'\"\n",
    "    json_data_dictionary['tables']['d_date_departures']['columns']['d_dd_weekday_short']['column_description'] \\\n",
    "        = \"Day of week in short form like 'Fri'\"\n",
    "    json_data_dictionary['tables']['d_date_departures']['columns']['d_dd_dayofweek']['column_description'] \\\n",
    "        = \"Day of week as number like '6'\"\n",
    "    json_data_dictionary['tables']['d_date_departures']['columns']['d_dd_day']['column_description'] \\\n",
    "        = \"Day number of current date like 2016-01-01 --> 1\"\n",
    "\n",
    "\n",
    "    # Table d_state_destinations --> To which states in the U.S. do immigrants want to continue their travel after\n",
    "    # their initial arrival and what demographics can immigrants expect when they arrive in the destination state, such\n",
    "    # as average temperature, population numbers or population density?\n",
    "    json_data_dictionary['tables']['d_state_destinations']['table_description'] \\\n",
    "        = \"To which State immigrants want to continue their travel after initial arrival in the U.S.\"\n",
    "    json_data_dictionary['tables']['d_state_destinations']['columns']['d_sd_id']['column_description'] \\\n",
    "        = \"PK of table d_state_destinations\"\n",
    "    json_data_dictionary['tables']['d_state_destinations']['columns']['d_sd_state_code']['column_description'] \\\n",
    "        = \"Abbreviation of State code\"\n",
    "    json_data_dictionary['tables']['d_state_destinations']['columns']['d_sd_state_name']['column_description'] \\\n",
    "        = \"Full name of State\"\n",
    "    json_data_dictionary['tables']['d_state_destinations']['columns']['d_sd_age_median']['column_description'] \\\n",
    "        = \"Median age of the population\"\n",
    "    json_data_dictionary['tables']['d_state_destinations']['columns']['d_sd_population_male']['column_description'] \\\n",
    "        = \"Average of male population\"\n",
    "    json_data_dictionary['tables']['d_state_destinations']['columns']['d_sd_population_female']['column_description'] \\\n",
    "        = \"Average of female population\"\n",
    "    json_data_dictionary['tables']['d_state_destinations']['columns']['d_sd_population_total']['column_description'] \\\n",
    "        = \"Average of population\"\n",
    "    json_data_dictionary['tables']['d_state_destinations']['columns']['d_sd_foreign_born']['column_description'] \\\n",
    "        = \"Average of the population born abroad\"\n",
    "\n",
    "    # Table f_i94_immigrations\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['table_description'] = \"I-94 Immigration data to the U.S.\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['f_i94_id']['column_description'] \\\n",
    "        = \"PK of table f_i94_immigrations\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['d_ia_id']['column_description'] \\\n",
    "        = \"FK of table d_immigration_airports\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['d_sd_id']['column_description'] \\\n",
    "        = \"FK of table d_state_destinations\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['d_da_id']['column_description'] \\\n",
    "        = \"FK of table d_date_arrivals\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['d_dd_id']['column_description'] \\\n",
    "        = \"FK of table d_date_departures\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['d_ic_id']['column_description'] \\\n",
    "        = \"FK of table d_immigration_countries\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['f_i94_cit']['column_description'] \\\n",
    "        = \"Country where the immigrants come from\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['f_i94_port']['column_description'] \\\n",
    "        = \"Arrival airport from immigrants to the U.S.\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['f_i94_addr']['column_description'] \\\n",
    "        = \"Location State where the immigrants want travel to\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['f_i94_arrdate_iso']['column_description'] \\\n",
    "        = \"Arrival date in the U.S.\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['f_i94_depdate_iso']['column_description'] \\\n",
    "        = \"Departure date from U.S.\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['f_i94_dtadfile']['column_description'] \\\n",
    "        = \"Date added to I-94 Files\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['f_i94_matflag']['column_description'] \\\n",
    "        = \"Match flag - Match of arrival and departure records\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['f_i94_count']['column_description'] \\\n",
    "        = \"Counter (1). This value is used for calculation purposes\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['f_i94_year']['column_description'] \\\n",
    "        = \"4 digit year when record added to I-94 Files\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['f_i94_month']['column_description'] \\\n",
    "        = \"Month when record added to I-94 Files\"\n",
    "    json_data_dictionary['tables']['f_i94_immigrations']['columns']['f_i94_port_state_code']['column_description'] \\\n",
    "        = \"State code of state where immigration airport (I94PORT) is located\"\n",
    "\n",
    "    return json_data_dictionary\n",
    "\n",
    "\n",
    "def persist_json_data(json_data, location_to_write):\n",
    "    # write data to file in json format\n",
    "    with open(location_to_write, \"w\") as outfile:\n",
    "        json.dump(json_data, outfile, sort_keys=True, indent=4, ensure_ascii=False)\n",
    "\n",
    "# File locations to get all columns of the source data to be described.\n",
    "locations_to_read = [\n",
    "    str(\"../P8_capstone_resource_files/parquet_star/PQ1/d_immigration_countries\")\n",
    "    , str(\"../P8_capstone_resource_files/parquet_star/PQ2/d_immigration_airports\")\n",
    "    , str(\"../P8_capstone_resource_files/parquet_star/PQ3/d_date_arrivals\")\n",
    "    , str(\"../P8_capstone_resource_files/parquet_star/PQ3/d_date_departures\")\n",
    "    , str(\"../P8_capstone_resource_files/parquet_star/PQ4/d_state_destinations\")\n",
    "    , str(\"../P8_capstone_resource_files/parquet_star/PQ4/f_i94_immigrations\")\n",
    "]\n",
    "\n",
    "def main():\n",
    "    # create automatically a data dictionary based on the loaded tables (data frames)\n",
    "    json_data = create_data_dictionary_from_df(locations_to_read)\n",
    "\n",
    "    # add descriptions to data dictionary for tables and table columns\n",
    "    json_data = update_descriptions(json_data)\n",
    "\n",
    "    # persist generated json_data to disk\n",
    "    location_to_write = \"../P8_capstone_documentation/10_P8_capstone_documentation_data_dictionary.json\"\n",
    "    persist_json_data(json_data, location_to_write)\n",
    "    print(\"Creation of data dictionary finished\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "* Outline of the steps taken in the project:\n",
    "    * Definition of the project scope: Four project questions had to be answered.\n",
    "    * The four given data sets from different areas were examined and aligned with the project questions. Based on these questions, the data model was built step by step.\n",
    "    * Examination of the data provided important insights. Pandas were used to take a quick look at small data sets to gain these insights.\n",
    "    * Transformation of the data within the ETL (Extract, Transform, Load) pipeline to build the star schema data model.\n",
    "    * Automatically creation of a data dictionary. The only manual part was to fill the table and column descriptions.\n",
    "\n",
    "\n",
    "* The purpose of the final data model is made explicit.\n",
    "    * At the beginning there were 4 project questions that had to be answered. Based on these questions, the data model\n",
    "      was built step by step to the final star data model.\n",
    "\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Used technologies and tools:\n",
    "    * This project uses Python, Pandas, Jupyter Notebook and Apache Spark (PySpark) in local mode to process 2016 U.S. immigration data.\n",
    "      There are 5 project questions to answer. The ETL pipeline described is always aligned with the questions to be\n",
    "      answered. The data model therefore evolves piece by piece to the final version. The specified tools were selected\n",
    "      because, on the one hand, they are easily suitable for data analysis and preparation. If the requirements become\n",
    "      larger and the amount of data increases, a switch to cloud technologies based on e.g. AWS is possible at any time.\n",
    "      However, this is not the scope of this project.\n",
    "\n",
    "* The write-up describes a logical approach to this project under the following scenarios:\n",
    "* Propose how often the data should be updated and why.\n",
    "    * The ETL process should run on a monthly basis.. This decision was made due to the fact that SAS data is only provided\n",
    "      monthly.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "    * The data was increased by 100x.\n",
    "        * Source data should be stored in Cloud storage like AWS S3\n",
    "        * To process all data in parallel use clustered Spark nodes (AWS EMR)\n",
    "        * Storing the calculated data in a Star Model data structure within a cloud-based data warehouse (DWH) such as\n",
    "          AWS Redshift, is possible. Optionally, it is also conceivable to store the Star Data Model as Parquet files in\n",
    "          S3 cloud storage for further analysis.\n",
    "\n",
    "    * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "        * The I94 source data should be read in daily. This will reduce the amount of data per run. Note that not every\n",
    "          project dataset (e.g. US Cities Demographics or Airport Codes) needs to be loaded daily.\n",
    "        * Apache Airflow could be used for the daily data loading procedure\n",
    "\n",
    "    * The database needed to be accessed by 100+ people.\n",
    "        * Output data should be stored in a cloud DWH such as AWS Redshift to be \"always available\".  In addition, there\n",
    "          is the possibility that the data in the Star data model is made available to the user for self-selection through\n",
    "          self-service BI. Tools such as QlikSense or similar can be used here.\n",
    "\n",
    "## Summary\n",
    "Project-Capstone provides tools to automatically process, clean, analyze US I94 Immigration data in a flexible way and\n",
    "help to answer questions like the five Project questions.\n",
    "\n",
    "--------------------------\n",
    "#### Hint: Call the script on a cluster with the given package:\n",
    "\n",
    "        !spark-submit --packages saurfang:spark-sas7bdat:2.1.0-s_2.11 script.py"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}