{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project 08 - Analysis of U.S. Immigration (I-94) Data\n",
    "### Udacity Data Engineer - Capstone Project\n",
    "> by Peter Wissel | 2021-05-05\n",
    "\n",
    "## Project Overview\n",
    "This project works with a data set for immigration to the United States. The supplementary datasets will include data on\n",
    "airport codes, U.S. city demographics and temperature data.\n",
    "\n",
    "The following process is divided into five sub-steps to illustrate how to answer the questions set by the business\n",
    "analytics team.\n",
    "\n",
    "The project file follows the following step:\n",
    "* Step 5: Complete Project Write Up\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "* Outline of the steps taken in the project:\n",
    "    * Definition of the project scope: Four project questions had to be answered.\n",
    "    * The four given data sets from different areas were examined and aligned with the project questions. Based on these\n",
    "      questions, the data model was built step by step.\n",
    "    * Examination of the data provided important insights. Pandas were used to take a quick look at small data sets to\n",
    "      gain these insights.\n",
    "    * Transformation of the data within the ETL (Extract, Transform, Load) pipeline to build the star schema data model.\n",
    "    * Automatically creation of a data dictionary. The only manual part was to fill the table and column descriptions.\n",
    "\n",
    "\n",
    "* The purpose of the final data model is made explicit.\n",
    "    * At the beginning there were 4 project questions that had to be answered. Based on these questions, the data model\n",
    "      was built step by step to the final star data model.\n",
    "\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Used technologies and tools:\n",
    "    * This project uses Python, Pandas, Jupyter Notebook and Apache Spark (PySpark) in local mode to process 2016 U.S. immigration data.\n",
    "      There are 5 project questions to answer. The ETL pipeline described is always aligned with the questions to be\n",
    "      answered. The data model therefore evolves piece by piece to the final version. The specified tools were selected\n",
    "      because, on the one hand, they are easily suitable for data analysis and preparation. If the requirements become\n",
    "      larger and the amount of data increases, a switch to cloud technologies based on e.g. AWS is possible at any time.\n",
    "      However, this is not the scope of this project.\n",
    "\n",
    "* The write-up describes a logical approach to this project under the following scenarios:\n",
    "* Propose how often the data should be updated and why.\n",
    "    * The ETL process should run on a monthly basis.. This decision was made due to the fact that SAS data is only provided\n",
    "      monthly.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "    * The data was increased by 100x.\n",
    "        * Source data should be stored in Cloud storage like AWS S3\n",
    "        * To process all data in parallel use clustered Spark nodes (AWS EMR)\n",
    "        * Storing the calculated data in a Star Model data structure within a cloud-based data warehouse (DWH) such as \n",
    "          AWS Redshift, is possible. Optionally, it is also conceivable to store the Star Data Model as Parquet files in \n",
    "          S3 cloud storage for further analysis. \n",
    "        \n",
    "    * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "        * The I94 source data should be read in daily. This will reduce the amount of data per run. Note that not every\n",
    "          project dataset (e.g. US Cities Demographics or Airport Codes) needs to be loaded daily.\n",
    "        * Apache Airflow could be used for the daily data loading procedure\n",
    "\n",
    "    * The database needed to be accessed by 100+ people.\n",
    "        * Output data should be stored in a cloud DWH such as AWS Redshift to be \"always available\".  In addition, there\n",
    "          is the possibility that the data in the Star data model is made available to the user for self-selection through\n",
    "          self-service BI. Tools such as QlikSense or similar can be used here.\n",
    "\n",
    "## Summary\n",
    "Project-Capstone provides tools to automatically process, clean, analyze US I94 Immigration data in a flexible way and\n",
    "help to answer questions like the four Project questions.\n",
    "\n",
    "--------------------------\n",
    "#### Hint: Call the script on a cluster with the given package:\n",
    "\n",
    "        !spark-submit --packages saurfang:spark-sas7bdat:2.1.0-s_2.11 script.py"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}